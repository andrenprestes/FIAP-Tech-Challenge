{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b477aaad-3eda-41cc-9d7e-9a609c64e80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.2-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\ffporto\\appdata\\local\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\ffporto\\appdata\\local\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-3.0.2-py3-none-win_amd64.whl (150.0 MB)\n",
      "   ---------------------------------------- 0.0/150.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.9/150.0 MB 18.1 MB/s eta 0:00:09\n",
      "   ---------------------------------------- 0.9/150.0 MB 18.1 MB/s eta 0:00:09\n",
      "   ---------------------------------------- 1.1/150.0 MB 9.7 MB/s eta 0:00:16\n",
      "   ---------------------------------------- 1.1/150.0 MB 9.7 MB/s eta 0:00:16\n",
      "   ---------------------------------------- 1.1/150.0 MB 9.7 MB/s eta 0:00:16\n",
      "   ---------------------------------------- 1.1/150.0 MB 9.7 MB/s eta 0:00:16\n",
      "   ---------------------------------------- 1.2/150.0 MB 3.7 MB/s eta 0:00:41\n",
      "   ---------------------------------------- 1.9/150.0 MB 5.6 MB/s eta 0:00:27\n",
      "   ---------------------------------------- 1.9/150.0 MB 5.6 MB/s eta 0:00:27\n",
      "    --------------------------------------- 3.3/150.0 MB 7.0 MB/s eta 0:00:21\n",
      "    --------------------------------------- 3.3/150.0 MB 7.1 MB/s eta 0:00:21\n",
      "    --------------------------------------- 3.3/150.0 MB 7.1 MB/s eta 0:00:21\n",
      "    --------------------------------------- 3.6/150.0 MB 6.4 MB/s eta 0:00:23\n",
      "    --------------------------------------- 3.6/150.0 MB 6.4 MB/s eta 0:00:23\n",
      "    --------------------------------------- 3.6/150.0 MB 6.4 MB/s eta 0:00:23\n",
      "    --------------------------------------- 3.6/150.0 MB 6.4 MB/s eta 0:00:23\n",
      "    --------------------------------------- 3.6/150.0 MB 6.4 MB/s eta 0:00:23\n",
      "    --------------------------------------- 3.7/150.0 MB 4.6 MB/s eta 0:00:32\n",
      "    --------------------------------------- 3.7/150.0 MB 4.6 MB/s eta 0:00:32\n",
      "    --------------------------------------- 3.7/150.0 MB 4.6 MB/s eta 0:00:32\n",
      "    --------------------------------------- 3.7/150.0 MB 4.6 MB/s eta 0:00:32\n",
      "    --------------------------------------- 3.7/150.0 MB 4.6 MB/s eta 0:00:32\n",
      "    --------------------------------------- 3.7/150.0 MB 4.6 MB/s eta 0:00:32\n",
      "    --------------------------------------- 3.7/150.0 MB 4.6 MB/s eta 0:00:32\n",
      "    --------------------------------------- 3.7/150.0 MB 4.6 MB/s eta 0:00:32\n",
      "   - -------------------------------------- 4.0/150.0 MB 3.4 MB/s eta 0:00:44\n",
      "   - -------------------------------------- 4.0/150.0 MB 3.4 MB/s eta 0:00:44\n",
      "   - -------------------------------------- 4.3/150.0 MB 3.3 MB/s eta 0:00:45\n",
      "   - -------------------------------------- 4.3/150.0 MB 3.3 MB/s eta 0:00:45\n",
      "   - -------------------------------------- 4.6/150.0 MB 3.3 MB/s eta 0:00:45\n",
      "   - -------------------------------------- 4.6/150.0 MB 3.3 MB/s eta 0:00:45\n",
      "   - -------------------------------------- 4.6/150.0 MB 3.3 MB/s eta 0:00:45\n",
      "   - -------------------------------------- 4.6/150.0 MB 3.3 MB/s eta 0:00:45\n",
      "   - -------------------------------------- 4.6/150.0 MB 3.3 MB/s eta 0:00:45\n",
      "   - -------------------------------------- 4.6/150.0 MB 2.8 MB/s eta 0:00:52\n",
      "   - -------------------------------------- 4.6/150.0 MB 2.8 MB/s eta 0:00:52\n",
      "   - -------------------------------------- 4.6/150.0 MB 2.8 MB/s eta 0:00:52\n",
      "   - -------------------------------------- 4.9/150.0 MB 2.8 MB/s eta 0:00:53\n",
      "   - -------------------------------------- 4.9/150.0 MB 2.8 MB/s eta 0:00:53\n",
      "   - -------------------------------------- 4.9/150.0 MB 2.8 MB/s eta 0:00:53\n",
      "   - -------------------------------------- 5.6/150.0 MB 3.0 MB/s eta 0:00:49\n",
      "   - -------------------------------------- 5.6/150.0 MB 3.0 MB/s eta 0:00:49\n",
      "   - -------------------------------------- 5.6/150.0 MB 3.0 MB/s eta 0:00:49\n",
      "   - -------------------------------------- 5.6/150.0 MB 3.0 MB/s eta 0:00:49\n",
      "   - -------------------------------------- 5.6/150.0 MB 3.0 MB/s eta 0:00:49\n",
      "   - -------------------------------------- 5.9/150.0 MB 2.8 MB/s eta 0:00:53\n",
      "   - -------------------------------------- 5.9/150.0 MB 2.8 MB/s eta 0:00:53\n",
      "   - -------------------------------------- 5.9/150.0 MB 2.8 MB/s eta 0:00:53\n",
      "   - -------------------------------------- 5.9/150.0 MB 2.6 MB/s eta 0:00:55\n",
      "   - -------------------------------------- 5.9/150.0 MB 2.6 MB/s eta 0:00:55\n",
      "   - -------------------------------------- 5.9/150.0 MB 2.6 MB/s eta 0:00:55\n",
      "   - -------------------------------------- 5.9/150.0 MB 2.6 MB/s eta 0:00:55\n",
      "   - -------------------------------------- 6.2/150.0 MB 2.5 MB/s eta 0:00:58\n",
      "   - -------------------------------------- 6.2/150.0 MB 2.5 MB/s eta 0:00:58\n",
      "   - -------------------------------------- 6.2/150.0 MB 2.5 MB/s eta 0:00:58\n",
      "   - -------------------------------------- 6.5/150.0 MB 2.5 MB/s eta 0:00:57\n",
      "   - -------------------------------------- 6.8/150.0 MB 2.6 MB/s eta 0:00:56\n",
      "   - -------------------------------------- 6.8/150.0 MB 2.6 MB/s eta 0:00:56\n",
      "   - -------------------------------------- 6.8/150.0 MB 2.5 MB/s eta 0:00:58\n",
      "   - -------------------------------------- 6.8/150.0 MB 2.5 MB/s eta 0:00:58\n",
      "   - -------------------------------------- 6.8/150.0 MB 2.5 MB/s eta 0:00:58\n",
      "   - -------------------------------------- 6.8/150.0 MB 2.5 MB/s eta 0:00:58\n",
      "   - -------------------------------------- 6.8/150.0 MB 2.5 MB/s eta 0:00:58\n",
      "   - -------------------------------------- 7.1/150.0 MB 2.4 MB/s eta 0:01:00\n",
      "   - -------------------------------------- 7.1/150.0 MB 2.4 MB/s eta 0:01:00\n",
      "   - -------------------------------------- 7.1/150.0 MB 2.4 MB/s eta 0:01:00\n",
      "   - -------------------------------------- 7.1/150.0 MB 2.4 MB/s eta 0:01:00\n",
      "   - -------------------------------------- 7.1/150.0 MB 2.4 MB/s eta 0:01:00\n",
      "   - -------------------------------------- 7.1/150.0 MB 2.2 MB/s eta 0:01:05\n",
      "   - -------------------------------------- 7.1/150.0 MB 2.2 MB/s eta 0:01:05\n",
      "   - -------------------------------------- 7.1/150.0 MB 2.2 MB/s eta 0:01:05\n",
      "   - -------------------------------------- 7.1/150.0 MB 2.2 MB/s eta 0:01:05\n",
      "   - -------------------------------------- 7.1/150.0 MB 2.2 MB/s eta 0:01:05\n",
      "   - -------------------------------------- 7.1/150.0 MB 2.2 MB/s eta 0:01:05\n",
      "   - -------------------------------------- 7.4/150.0 MB 2.1 MB/s eta 0:01:07\n",
      "   - -------------------------------------- 7.4/150.0 MB 2.1 MB/s eta 0:01:07\n",
      "   - -------------------------------------- 7.4/150.0 MB 2.1 MB/s eta 0:01:09\n",
      "   -- ------------------------------------- 7.7/150.0 MB 2.1 MB/s eta 0:01:07\n",
      "   -- ------------------------------------- 8.0/150.0 MB 2.2 MB/s eta 0:01:05\n",
      "   -- ------------------------------------- 8.0/150.0 MB 2.2 MB/s eta 0:01:05\n",
      "   -- ------------------------------------- 8.3/150.0 MB 2.2 MB/s eta 0:01:04\n",
      "   -- ------------------------------------- 8.3/150.0 MB 2.2 MB/s eta 0:01:04\n",
      "   -- ------------------------------------- 8.3/150.0 MB 2.2 MB/s eta 0:01:04\n",
      "   -- ------------------------------------- 8.3/150.0 MB 2.1 MB/s eta 0:01:07\n",
      "   -- ------------------------------------- 8.6/150.0 MB 2.2 MB/s eta 0:01:06\n",
      "   -- ------------------------------------- 8.6/150.0 MB 2.2 MB/s eta 0:01:06\n",
      "   -- ------------------------------------- 8.6/150.0 MB 2.1 MB/s eta 0:01:07\n",
      "   -- ------------------------------------- 9.2/150.0 MB 2.2 MB/s eta 0:01:03\n",
      "   -- ------------------------------------- 9.2/150.0 MB 2.2 MB/s eta 0:01:03\n",
      "   -- ------------------------------------- 9.5/150.0 MB 2.3 MB/s eta 0:01:02\n",
      "   -- ------------------------------------- 9.8/150.0 MB 2.3 MB/s eta 0:01:01\n",
      "   -- ------------------------------------- 9.8/150.0 MB 2.3 MB/s eta 0:01:02\n",
      "   -- ------------------------------------- 10.7/150.0 MB 2.4 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 11.0/150.0 MB 2.4 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 11.0/150.0 MB 2.4 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 11.0/150.0 MB 2.4 MB/s eta 0:00:59\n",
      "   --- ------------------------------------ 11.8/150.0 MB 2.4 MB/s eta 0:00:57\n",
      "   --- ------------------------------------ 12.8/150.0 MB 2.5 MB/s eta 0:00:56\n",
      "   --- ------------------------------------ 12.9/150.0 MB 2.5 MB/s eta 0:00:56\n",
      "   --- ------------------------------------ 13.3/150.0 MB 2.4 MB/s eta 0:00:57\n",
      "   --- ------------------------------------ 13.7/150.0 MB 2.5 MB/s eta 0:00:55\n",
      "   --- ------------------------------------ 13.8/150.0 MB 2.5 MB/s eta 0:00:56\n",
      "   --- ------------------------------------ 13.8/150.0 MB 2.5 MB/s eta 0:00:56\n",
      "   --- ------------------------------------ 13.8/150.0 MB 2.5 MB/s eta 0:00:56\n",
      "   --- ------------------------------------ 14.0/150.0 MB 2.8 MB/s eta 0:00:50\n",
      "   --- ------------------------------------ 14.0/150.0 MB 2.8 MB/s eta 0:00:50\n",
      "   --- ------------------------------------ 14.1/150.0 MB 2.7 MB/s eta 0:00:51\n",
      "   --- ------------------------------------ 14.1/150.0 MB 2.7 MB/s eta 0:00:51\n",
      "   --- ------------------------------------ 14.1/150.0 MB 2.7 MB/s eta 0:00:51\n",
      "   --- ------------------------------------ 14.1/150.0 MB 2.7 MB/s eta 0:00:51\n",
      "   --- ------------------------------------ 14.1/150.0 MB 2.7 MB/s eta 0:00:51\n",
      "   --- ------------------------------------ 14.4/150.0 MB 2.6 MB/s eta 0:00:52\n",
      "   --- ------------------------------------ 14.4/150.0 MB 2.6 MB/s eta 0:00:52\n",
      "   --- ------------------------------------ 14.9/150.0 MB 2.9 MB/s eta 0:00:48\n",
      "   --- ------------------------------------ 14.9/150.0 MB 2.9 MB/s eta 0:00:48\n",
      "   --- ------------------------------------ 14.9/150.0 MB 2.9 MB/s eta 0:00:48\n",
      "   ---- ----------------------------------- 16.2/150.0 MB 3.4 MB/s eta 0:00:40\n",
      "   ---- ----------------------------------- 16.2/150.0 MB 3.3 MB/s eta 0:00:41\n",
      "   ---- ----------------------------------- 16.6/150.0 MB 3.4 MB/s eta 0:00:39\n",
      "   ---- ----------------------------------- 16.6/150.0 MB 3.4 MB/s eta 0:00:39\n",
      "   ---- ----------------------------------- 16.6/150.0 MB 3.4 MB/s eta 0:00:39\n",
      "   ---- ----------------------------------- 16.6/150.0 MB 3.4 MB/s eta 0:00:39\n",
      "   ---- ----------------------------------- 16.6/150.0 MB 3.4 MB/s eta 0:00:39\n",
      "   ---- ----------------------------------- 17.1/150.0 MB 3.6 MB/s eta 0:00:37\n",
      "   ---- ----------------------------------- 17.1/150.0 MB 3.6 MB/s eta 0:00:37\n",
      "   ---- ----------------------------------- 17.1/150.0 MB 3.6 MB/s eta 0:00:37\n",
      "   ---- ----------------------------------- 17.1/150.0 MB 3.6 MB/s eta 0:00:37\n",
      "   ---- ----------------------------------- 17.4/150.0 MB 4.0 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 17.4/150.0 MB 4.0 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 17.4/150.0 MB 4.0 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 17.4/150.0 MB 4.0 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 17.4/150.0 MB 4.0 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 17.4/150.0 MB 4.0 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 17.4/150.0 MB 4.0 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 17.4/150.0 MB 4.0 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 17.4/150.0 MB 4.0 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 17.4/150.0 MB 4.0 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 17.7/150.0 MB 3.5 MB/s eta 0:00:38\n",
      "   ---- ----------------------------------- 17.7/150.0 MB 3.5 MB/s eta 0:00:38\n",
      "   ---- ----------------------------------- 17.7/150.0 MB 3.5 MB/s eta 0:00:38\n",
      "   ---- ----------------------------------- 17.7/150.0 MB 3.5 MB/s eta 0:00:38\n",
      "   ---- ----------------------------------- 17.7/150.0 MB 3.5 MB/s eta 0:00:38\n",
      "   ---- ----------------------------------- 17.7/150.0 MB 3.5 MB/s eta 0:00:38\n",
      "   ---- ----------------------------------- 17.9/150.0 MB 3.2 MB/s eta 0:00:42\n",
      "   ---- ----------------------------------- 17.9/150.0 MB 3.2 MB/s eta 0:00:42\n",
      "   ---- ----------------------------------- 18.0/150.0 MB 3.1 MB/s eta 0:00:43\n",
      "   ---- ----------------------------------- 18.6/150.0 MB 3.3 MB/s eta 0:00:40\n",
      "   ----- ---------------------------------- 19.0/150.0 MB 3.4 MB/s eta 0:00:40\n",
      "   ----- ---------------------------------- 19.4/150.0 MB 3.3 MB/s eta 0:00:40\n",
      "   ----- ---------------------------------- 19.6/150.0 MB 3.3 MB/s eta 0:00:40\n",
      "   ----- ---------------------------------- 20.2/150.0 MB 3.4 MB/s eta 0:00:39\n",
      "   ----- ---------------------------------- 20.5/150.0 MB 3.4 MB/s eta 0:00:39\n",
      "   ----- ---------------------------------- 20.7/150.0 MB 3.3 MB/s eta 0:00:39\n",
      "   ----- ---------------------------------- 21.0/150.0 MB 3.3 MB/s eta 0:00:40\n",
      "   ------ --------------------------------- 22.6/150.0 MB 3.4 MB/s eta 0:00:38\n",
      "   ------ --------------------------------- 24.1/150.0 MB 3.8 MB/s eta 0:00:34\n",
      "   ------ --------------------------------- 25.5/150.0 MB 4.7 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 27.9/150.0 MB 12.1 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 30.2/150.0 MB 23.4 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 31.0/150.0 MB 29.7 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 31.8/150.0 MB 32.7 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 32.8/150.0 MB 31.2 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 33.7/150.0 MB 28.4 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 34.7/150.0 MB 27.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 35.7/150.0 MB 27.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 37.2/150.0 MB 26.2 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 38.8/150.0 MB 25.2 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 40.4/150.0 MB 24.2 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 41.5/150.0 MB 25.2 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 42.6/150.0 MB 25.1 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 43.5/150.0 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 44.4/150.0 MB 26.2 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 45.0/150.0 MB 25.1 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 45.3/150.0 MB 24.2 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 45.6/150.0 MB 21.9 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 45.8/150.0 MB 21.1 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 46.0/150.0 MB 18.7 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 46.2/150.0 MB 17.7 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 46.4/150.0 MB 16.4 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 46.9/150.0 MB 16.0 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 47.1/150.0 MB 14.6 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 47.7/150.0 MB 13.9 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 48.5/150.0 MB 13.9 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 49.2/150.0 MB 13.4 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 50.3/150.0 MB 13.1 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 50.9/150.0 MB 12.6 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 51.8/150.0 MB 12.6 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 52.6/150.0 MB 12.4 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 53.2/150.0 MB 12.1 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 54.0/150.0 MB 11.9 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 55.1/150.0 MB 12.1 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 55.8/150.0 MB 13.4 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 56.8/150.0 MB 16.4 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 57.5/150.0 MB 17.7 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 58.3/150.0 MB 17.7 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 59.0/150.0 MB 17.7 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 59.8/150.0 MB 17.2 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 60.7/150.0 MB 17.2 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 61.5/150.0 MB 17.2 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 62.4/150.0 MB 17.7 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 63.3/150.0 MB 18.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 64.4/150.0 MB 18.7 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 65.5/150.0 MB 18.7 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 66.5/150.0 MB 19.3 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 67.6/150.0 MB 19.3 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 68.6/150.0 MB 19.8 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 69.8/150.0 MB 21.1 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 69.9/150.0 MB 21.1 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 69.9/150.0 MB 21.1 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 69.9/150.0 MB 21.1 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 70.0/150.0 MB 15.6 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 72.0/150.0 MB 17.7 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 73.8/150.0 MB 18.7 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 75.9/150.0 MB 19.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 76.9/150.0 MB 19.3 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 77.8/150.0 MB 18.7 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 78.9/150.0 MB 19.3 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 79.8/150.0 MB 19.3 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 80.8/150.0 MB 27.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 81.5/150.0 MB 25.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 82.3/150.0 MB 23.4 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 82.8/150.0 MB 21.8 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 83.4/150.0 MB 20.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 84.5/150.0 MB 20.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 85.1/150.0 MB 19.3 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 85.7/150.0 MB 18.2 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 86.6/150.0 MB 17.7 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 87.6/150.0 MB 18.2 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 88.7/150.0 MB 17.7 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 89.6/150.0 MB 17.7 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 90.5/150.0 MB 17.7 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 91.2/150.0 MB 17.3 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 91.6/150.0 MB 16.8 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 91.9/150.0 MB 16.0 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 92.1/150.0 MB 15.6 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 92.6/150.0 MB 15.2 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 93.5/150.0 MB 16.0 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 94.4/150.0 MB 15.6 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 95.3/150.0 MB 15.2 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 96.2/150.0 MB 16.0 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 96.9/150.0 MB 16.0 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 97.8/150.0 MB 16.0 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 98.4/150.0 MB 15.2 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 99.2/150.0 MB 15.2 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 99.9/150.0 MB 14.9 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 100.6/150.0 MB 14.6 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 101.4/150.0 MB 15.2 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 102.4/150.0 MB 17.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 102.6/150.0 MB 18.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 102.6/150.0 MB 18.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 102.6/150.0 MB 18.2 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 104.5/150.0 MB 15.6 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 107.3/150.0 MB 18.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 108.5/150.0 MB 19.3 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 109.6/150.0 MB 20.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 110.7/150.0 MB 21.1 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 111.9/150.0 MB 21.8 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 112.8/150.0 MB 22.6 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 114.0/150.0 MB 31.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 115.1/150.0 MB 28.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 116.1/150.0 MB 26.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 117.0/150.0 MB 24.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 117.7/150.0 MB 22.6 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 118.3/150.0 MB 21.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 118.8/150.0 MB 21.1 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 119.4/150.0 MB 19.3 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 119.9/150.0 MB 19.3 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 120.6/150.0 MB 18.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 121.7/150.0 MB 18.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 122.5/150.0 MB 17.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 123.5/150.0 MB 17.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 124.3/150.0 MB 17.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 125.0/150.0 MB 16.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 125.5/150.0 MB 16.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 126.2/150.0 MB 15.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 127.2/150.0 MB 15.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 128.3/150.0 MB 16.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 129.5/150.0 MB 18.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 130.7/150.0 MB 19.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 132.0/150.0 MB 19.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 133.1/150.0 MB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 134.3/150.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 135.5/150.0 MB 22.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 136.4/150.0 MB 24.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 137.0/150.0 MB 23.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 137.4/150.0 MB 21.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 138.3/150.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 139.2/150.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 140.3/150.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 141.7/150.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 142.8/150.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 144.0/150.0 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 144.5/150.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 146.1/150.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 146.1/150.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 146.1/150.0 MB 21.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  146.6/150.0 MB 17.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  149.2/150.0 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  149.6/150.0 MB 21.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 150.0/150.0 MB 4.3 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41785409-7bd3-44c8-a959-082b8c289457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuição antes do SMOTE:\n",
      "contratado\n",
      "0    29988\n",
      "1     1578\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97     12852\n",
      "           1       0.47      0.19      0.27       677\n",
      "\n",
      "    accuracy                           0.95     13529\n",
      "   macro avg       0.71      0.59      0.62     13529\n",
      "weighted avg       0.93      0.95      0.94     13529\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[12706   146]\n",
      " [  549   128]]\n",
      "\n",
      "ROC AUC: 0.789143336638775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ffporto\\AppData\\Local\\anaconda3\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/07/15 14:02:24 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5e7a89e2644d45ab51990f0c2875e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelos treinados e salvos com sucesso!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def carregar_dados(path):\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def extrair_features_completas(df):\n",
    "    df.columns = df.columns.astype(str)\n",
    "    # Vetorização textual combinada de CV, objetivo e atividades da vaga\n",
    "    texto_completo = (\n",
    "        df['cv'].fillna('') + ' ' +\n",
    "        df['objetivo_profissional'].fillna('') + ' ' +\n",
    "        df['titulo_profissional'].fillna('') + ' ' +\n",
    "        df['principais_atividades_vaga'].fillna('')\n",
    "    )\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=500)\n",
    "    X_texto = tfidf.fit_transform(texto_completo)\n",
    "\n",
    "    # Selecionar todas as colunas one-hot e numéricas, incluindo as de match\n",
    "    X_estrut = df.filter(\n",
    "        regex=r'^(tipo_contratacao_|nivel_profissional_|nivel_academico_|nivel_ingles_|nivel_espanhol_|ingles_vaga_|espanhol_vaga_|feature_mesma_cidade$|^match_)'\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Concatenar tudo\n",
    "    X_final = pd.concat([pd.DataFrame(X_texto.toarray()), X_estrut.reset_index(drop=True)], axis=1)\n",
    "    return X_final, tfidf\n",
    "\n",
    "def treinar_modelo_supervisionado(df):\n",
    "    df.columns = df.columns.astype(str)\n",
    "    X, tfidf = extrair_features_completas(df)\n",
    "    X.columns = X.columns.astype(str)\n",
    "    y = df['contratado']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    print(\"Distribuição antes do SMOTE:\")\n",
    "    print(y_train.value_counts())\n",
    "    scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "\n",
    "    # smote = SMOTE(random_state=42, sampling_strategy=0.5, k_neighbors=3)\n",
    "    # X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # print(\"\\nDistribuição após SMOTE:\")\n",
    "    # print(y_train_bal.value_counts())\n",
    "\n",
    "    # scale_pos_weight = len(y_train_bal[y_train_bal == 0]) / len(y_train_bal[y_train_bal == 1])\n",
    "    clf = XGBClassifier(\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        max_depth=32,\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=300,\n",
    "        max_delta_step=10,\n",
    "        eta=0.1,\n",
    "        subsample=0.5,\n",
    "        eval_metric='auc',\n",
    "        nthread=16,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        objective='binary:logitraw'\n",
    "    )\n",
    "    # clf.fit(X_train_bal, y_train_bal)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nROC AUC:\", roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))\n",
    "\n",
    "    mlflow.set_experiment(\"modelo_candidato_sucesso\")\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(clf.get_params())\n",
    "        # Métricas adicionais\n",
    "        mlflow.log_metric(\"acuracia\", clf.score(X_test, y_test))\n",
    "        mlflow.log_metric(\"precision\", precision_score(y_test, y_pred))\n",
    "        mlflow.log_metric(\"recall\", recall_score(y_test, y_pred))\n",
    "        mlflow.log_metric(\"f1_score\", f1_score(y_test, y_pred))\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))\n",
    "        \n",
    "        # Feature importance\n",
    "        importances = clf.feature_importances_\n",
    "        feature_names = X.columns.tolist()\n",
    "        fi_df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
    "        fi_df.to_csv(\"feature_importances.csv\", index=False)\n",
    "        mlflow.log_artifact(\"feature_importances.csv\")\n",
    "        \n",
    "        # Input example e assinatura\n",
    "        input_example = X_test.iloc[:1]\n",
    "        signature = infer_signature(X_test, clf.predict(X_test))\n",
    "        mlflow.sklearn.log_model(clf, \"modelo_xgboost\", input_example=input_example, signature=signature)\n",
    "\n",
    "    joblib.dump(clf, \"modelo_xgboost.pkl\")\n",
    "    joblib.dump(tfidf, \"vetorizador_tfidf.pkl\")\n",
    "\n",
    "    return clf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"C:\\\\Users\\\\ffporto\\\\Desktop\\\\Estudo\\\\FIAP\\\\fase05\\\\data\\\\\"\n",
    "    df = carregar_dados(f\"{path}dataset_processado.parquet\")\n",
    "    df.columns = df.columns.astype(str)\n",
    "    clf = treinar_modelo_supervisionado(df)\n",
    "    df.to_parquet(f\"{path}dataset_clusterizado.parquet\", index=False)\n",
    "    print(\"Modelos treinados e salvos com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0282f1f5-9b2e-4e38-83e2-2fa30e12fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score, precision_recall_curve\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier, cv, DMatrix\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def carregar_dados(path):\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def extrair_features_completas(df):\n",
    "    df.columns = df.columns.astype(str)\n",
    "    texto_completo = (\n",
    "        df['cv'].fillna('') + ' ' +\n",
    "        df['objetivo_profissional'].fillna('') + ' ' +\n",
    "        df['titulo_profissional'].fillna('') + ' ' +\n",
    "        df['principais_atividades_vaga'].fillna('')\n",
    "    )\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=100)\n",
    "    X_texto = tfidf.fit_transform(texto_completo)\n",
    "\n",
    "    X_estrut = df.filter(\n",
    "        regex=r'^(tipo_contratacao_|nivel_profissional_|nivel_academico_|nivel_ingles_|nivel_espanhol_|ingles_vaga_|espanhol_vaga_|feature_mesma_cidade$|^match_|^qtd_keywords_cv$|^sim_cv_atividade$)'\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    X_final = pd.concat([pd.DataFrame(X_texto.toarray()), X_estrut.reset_index(drop=True)], axis=1)\n",
    "    return X_final, tfidf\n",
    "\n",
    "def fpreproc(dtrain, dtest, param):\n",
    "    label = dtrain.get_label()\n",
    "    ratio = float(np.sum(label == 0)) / np.sum(label == 1)\n",
    "    param['scale_pos_weight'] = ratio\n",
    "    wtrain = dtrain.get_weight()\n",
    "    wtest = dtest.get_weight()\n",
    "    sum_weight = sum(wtrain) + sum(wtest)\n",
    "    if sum(wtrain) > 0:\n",
    "        wtrain *= sum_weight / sum(wtrain)\n",
    "    if sum(wtest) > 0:\n",
    "        wtest *= sum_weight / sum(wtest)\n",
    "    dtrain.set_weight(wtrain)\n",
    "    dtest.set_weight(wtest)\n",
    "    return dtrain, dtest, param\n",
    "\n",
    "def plot_threshold_curve(y_test, y_probs):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    best_thresh = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(thresholds, f1_scores[:-1], label=\"F1 Score\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.title(\"F1 Score vs Threshold\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"f1_threshold_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Melhor threshold para F1: {best_thresh:.2f}\")\n",
    "    return best_thresh\n",
    "\n",
    "\n",
    "def grid_search_xgboost(X_train, y_train):\n",
    "    param_grid = {\n",
    "        'max_depth': [8, 16, 32],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_delta_step': [5,10,20],\n",
    "        'n_estimators': [150, 250, 350],\n",
    "        'nthread': [8,16,32],\n",
    "        'subsample': [0.5, 0.7, 0.8],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "    }\n",
    "    clf = XGBClassifier(scale_pos_weight=float(np.sum(y_train == 0)) / np.sum(y_train == 1),\n",
    "                        eval_metric='auc',\n",
    "                        objective='binary:logistic',\n",
    "                        use_label_encoder=False,\n",
    "                        random_state=42)\n",
    "    grid = GridSearchCV(clf, param_grid, scoring='roc_auc', cv=3, verbose=1, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(\"Melhores parâmetros do GridSearch:\", grid.best_params_)\n",
    "    return grid.best_estimator_\n",
    "\n",
    "def treinar_modelo_supervisionado(df):\n",
    "    df.columns = df.columns.astype(str)\n",
    "    X, tfidf = extrair_features_completas(df)\n",
    "    X.columns = X.columns.astype(str)\n",
    "    y = df['contratado']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    print(\"Distribuição original:\")\n",
    "    print(y_train.value_counts())\n",
    "\n",
    "    dtrain = DMatrix(X_train, label=y_train)\n",
    "    dtest = DMatrix(X_test, label=y_test)\n",
    "\n",
    "    param = {\n",
    "        'max_depth': 32,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_delta_step': 10,\n",
    "        'n_estimators': 300,\n",
    "        'nthread': 16,\n",
    "        'eta': 0.1,\n",
    "        'subsample': 0.5,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'seed': 42\n",
    "    }\n",
    "    num_round = 300\n",
    "\n",
    "    print(\"\\nExecutando cross-validation com xgb.cv...\")\n",
    "    cv_results = cv(\n",
    "        param,\n",
    "        dtrain,\n",
    "        num_boost_round=num_round,\n",
    "        nfold=5,\n",
    "        seed=42,\n",
    "        metrics=['auc'],\n",
    "        fpreproc=fpreproc,\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=10\n",
    "    )\n",
    "\n",
    "    best_num_round = len(cv_results)\n",
    "    print(f\"\\nMelhor número de rounds: {best_num_round}\")\n",
    "\n",
    "    clf = grid_search_xgboost(X_train, y_train)\n",
    "    clf.set_params(n_estimators=best_num_round)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_probs = clf.predict_proba(X_test)[:, 1]\n",
    "    best_thresh = plot_threshold_curve(y_test, y_probs)\n",
    "    y_pred = (y_probs >= best_thresh).astype(int)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nROC AUC:\", roc_auc_score(y_test, y_probs))\n",
    "\n",
    "    mlflow.set_experiment(\"modelo_candidato_sucesso\")\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(clf.get_params())\n",
    "        mlflow.log_metric(\"acuracia\", clf.score(X_test, y_test))\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc_score(y_test, y_probs))\n",
    "        mlflow.log_metric(\"precision\", precision_score(y_test, y_pred))\n",
    "        mlflow.log_metric(\"recall\", recall_score(y_test, y_pred))\n",
    "        mlflow.log_metric(\"f1_score\", f1_score(y_test, y_pred))\n",
    "        mlflow.log_artifact(\"f1_threshold_plot.png\")\n",
    "\n",
    "        importances = clf.feature_importances_\n",
    "        feature_names = X.columns.tolist()\n",
    "        fi_df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
    "        fi_df.to_csv(\"feature_importances.csv\", index=False)\n",
    "        mlflow.log_artifact(\"feature_importances.csv\")\n",
    "\n",
    "        input_example = X_test.iloc[:1]\n",
    "        signature = infer_signature(X_test, clf.predict(X_test))\n",
    "        mlflow.sklearn.log_model(clf, \"modelo_xgboost\", input_example=input_example, signature=signature)\n",
    "\n",
    "    joblib.dump(clf, \"modelo_xgboost.pkl\")\n",
    "    joblib.dump(tfidf, \"vetorizador_tfidf.pkl\")\n",
    "\n",
    "    return clf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"C:\\\\Users\\\\ffporto\\\\Desktop\\\\Estudo\\\\FIAP\\\\fase05\\\\data\\\\\"\n",
    "    df = carregar_dados(f\"{path}dataset_processado.parquet\")\n",
    "    df.columns = df.columns.astype(str)\n",
    "    clf = treinar_modelo_supervisionado(df)\n",
    "    df.to_parquet(f\"{path}dataset_clusterizado.parquet\", index=False)\n",
    "    print(\"Modelos treinados e salvos com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bcf79f01-8106-42ff-9487-b910d8c8c415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuição original do treinamento:\n",
      "contratado\n",
      "0    29988\n",
      "1     1578\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribuição após SMOTE no treinamento:\n",
      "contratado\n",
      "1    29528\n",
      "0    23742\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Executando cross-validation com xgb.cv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ffporto\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:209: UserWarning: [08:08:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  return getattr(self.bst, name)(*args, **kwargs)\n",
      "C:\\Users\\ffporto\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:215: UserWarning: [08:08:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.bst.update(self.dtrain, iteration, fobj)\n",
      "C:\\Users\\ffporto\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:215: UserWarning: [08:08:26] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.bst.update(self.dtrain, iteration, fobj)\n",
      "C:\\Users\\ffporto\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:215: UserWarning: [08:08:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.bst.update(self.dtrain, iteration, fobj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.86939+0.00292\ttest-auc:0.86127+0.00524\n",
      "[10]\ttrain-auc:0.96273+0.00112\ttest-auc:0.95483+0.00340\n",
      "[20]\ttrain-auc:0.97711+0.00088\ttest-auc:0.97061+0.00238\n",
      "[30]\ttrain-auc:0.98333+0.00034\ttest-auc:0.97743+0.00181\n",
      "[40]\ttrain-auc:0.98662+0.00032\ttest-auc:0.98117+0.00166\n",
      "[50]\ttrain-auc:0.98914+0.00029\ttest-auc:0.98397+0.00144\n",
      "[60]\ttrain-auc:0.99114+0.00019\ttest-auc:0.98623+0.00127\n",
      "[70]\ttrain-auc:0.99275+0.00016\ttest-auc:0.98800+0.00116\n",
      "[80]\ttrain-auc:0.99399+0.00013\ttest-auc:0.98941+0.00108\n",
      "[90]\ttrain-auc:0.99502+0.00009\ttest-auc:0.99065+0.00096\n",
      "[100]\ttrain-auc:0.99577+0.00011\ttest-auc:0.99154+0.00084\n",
      "[110]\ttrain-auc:0.99640+0.00008\ttest-auc:0.99228+0.00077\n",
      "[120]\ttrain-auc:0.99687+0.00008\ttest-auc:0.99283+0.00073\n",
      "[130]\ttrain-auc:0.99731+0.00004\ttest-auc:0.99338+0.00070\n",
      "[140]\ttrain-auc:0.99768+0.00004\ttest-auc:0.99384+0.00069\n",
      "[150]\ttrain-auc:0.99797+0.00004\ttest-auc:0.99421+0.00065\n",
      "[160]\ttrain-auc:0.99821+0.00004\ttest-auc:0.99449+0.00063\n",
      "[170]\ttrain-auc:0.99844+0.00005\ttest-auc:0.99476+0.00062\n",
      "[180]\ttrain-auc:0.99863+0.00005\ttest-auc:0.99501+0.00058\n",
      "[190]\ttrain-auc:0.99878+0.00005\ttest-auc:0.99521+0.00055\n",
      "[200]\ttrain-auc:0.99891+0.00004\ttest-auc:0.99539+0.00052\n",
      "[210]\ttrain-auc:0.99904+0.00005\ttest-auc:0.99556+0.00051\n",
      "[220]\ttrain-auc:0.99914+0.00004\ttest-auc:0.99568+0.00049\n",
      "[230]\ttrain-auc:0.99923+0.00004\ttest-auc:0.99582+0.00049\n",
      "[240]\ttrain-auc:0.99931+0.00004\ttest-auc:0.99595+0.00047\n",
      "[250]\ttrain-auc:0.99939+0.00004\ttest-auc:0.99609+0.00047\n",
      "[260]\ttrain-auc:0.99946+0.00003\ttest-auc:0.99620+0.00046\n",
      "[270]\ttrain-auc:0.99952+0.00003\ttest-auc:0.99630+0.00046\n",
      "[280]\ttrain-auc:0.99958+0.00003\ttest-auc:0.99638+0.00046\n",
      "[290]\ttrain-auc:0.99963+0.00002\ttest-auc:0.99646+0.00047\n",
      "[299]\ttrain-auc:0.99966+0.00002\ttest-auc:0.99651+0.00047\n",
      "\n",
      "Melhor número de rounds: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ffporto\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:10:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96     12852\n",
      "           1       0.29      0.40      0.34       677\n",
      "\n",
      "    accuracy                           0.92     13529\n",
      "   macro avg       0.63      0.67      0.65     13529\n",
      "weighted avg       0.93      0.92      0.93     13529\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[12191   661]\n",
      " [  406   271]]\n",
      "\n",
      "ROC AUC: 0.8012708365801597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ffporto\\AppData\\Local\\anaconda3\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/07/16 08:11:21 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3ffdc2f3af47c482ff877fb51cd3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelos treinados e salvos com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier, cv, DMatrix\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def carregar_dados(path):\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def extrair_features_completas(df):\n",
    "    df.columns = df.columns.astype(str)\n",
    "    texto_completo = (\n",
    "        df['cv'].fillna('') + ' ' +\n",
    "        df['objetivo_profissional'].fillna('') + ' ' +\n",
    "        df['titulo_profissional'].fillna('') + ' ' +\n",
    "        df['principais_atividades_vaga'].fillna('')\n",
    "    )\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=100)\n",
    "    X_texto = tfidf.fit_transform(texto_completo)\n",
    "\n",
    "    X_estrut = df.filter(\n",
    "       regex=r'^(tipo_contratacao_|nivel_profissional_|nivel_academico_|nivel_ingles_|nivel_espanhol_|ingles_vaga_|espanhol_vaga_|feature_mesma_cidade$|^match_|^qtd_keywords_cv$|^sim_cv_atividade$)'\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    X_final = pd.concat([pd.DataFrame(X_texto.toarray()), X_estrut.reset_index(drop=True)], axis=1)\n",
    "    return X_final, tfidf\n",
    "\n",
    "def fpreproc(dtrain, dtest, param):\n",
    "    label = dtrain.get_label()\n",
    "    ratio = float(np.sum(label == 0)) / np.sum(label == 1)\n",
    "    param['scale_pos_weight'] = ratio\n",
    "    wtrain = dtrain.get_weight()\n",
    "    wtest = dtest.get_weight()\n",
    "    sum_weight = sum(wtrain) + sum(wtest)\n",
    "    if sum(wtrain) > 0:\n",
    "        wtrain *= sum_weight / sum(wtrain)\n",
    "    if sum(wtest) > 0:\n",
    "        wtest *= sum_weight / sum(wtest)\n",
    "    dtrain.set_weight(wtrain)\n",
    "    dtest.set_weight(wtest)\n",
    "    return dtrain, dtest, param\n",
    "\n",
    "def treinar_modelo_supervisionado(df):\n",
    "    df.columns = df.columns.astype(str)\n",
    "    X, tfidf = extrair_features_completas(df)\n",
    "    X.columns = X.columns.astype(str)\n",
    "    y = df['contratado']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    print(\"Distribuição original do treinamento:\")\n",
    "    print(y_train.value_counts())\n",
    "\n",
    "    # Aplicar SMOTE APENAS no conjunto de treinamento\n",
    "    smoteenn = SMOTEENN(random_state=42)\n",
    "    X_train_res, y_train_res = smoteenn.fit_resample(X_train, y_train)\n",
    "\n",
    "    print(\"\\nDistribuição após SMOTE no treinamento:\")\n",
    "    print(y_train_res.value_counts())\n",
    "\n",
    "    dtrain = DMatrix(X_train_res, label=y_train_res)\n",
    "    dtest = DMatrix(X_test, label=y_test) # O dtest não deve ser reamostrado\n",
    "    \n",
    "    param = {\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_delta_step': 1,\n",
    "        'n_estimators': 300,\n",
    "        'nthread': 16,\n",
    "        # 'eta': 0.1,\n",
    "        'subsample': 0.5,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'seed': 42\n",
    "    }\n",
    "    num_round = param['n_estimators']\n",
    "\n",
    "    print(\"\\nExecutando cross-validation com xgb.cv...\")\n",
    "    cv_results = cv(\n",
    "        param,\n",
    "        dtrain,\n",
    "        num_boost_round=num_round,\n",
    "        nfold=5,\n",
    "        seed=42,\n",
    "        metrics=['auc'],\n",
    "        fpreproc=fpreproc,\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=10\n",
    "    )\n",
    "\n",
    "    best_num_round = len(cv_results)\n",
    "    print(f\"\\nMelhor número de rounds: {best_num_round}\")\n",
    "\n",
    "    clf = XGBClassifier(\n",
    "        max_depth=param['max_depth'],\n",
    "        learning_rate=param['learning_rate'],\n",
    "        max_delta_step=param['max_delta_step'],\n",
    "        n_estimators=best_num_round,\n",
    "        nthread=param['nthread'],\n",
    "        # eta=param['eta'],\n",
    "        subsample=param['subsample'],\n",
    "        colsample_bytree=param['colsample_bytree'],\n",
    "        # scale_pos_weight=float(np.sum(y_train == 0)) / np.sum(y_train == 1),\n",
    "        objective=param['objective'],\n",
    "        eval_metric=param['eval_metric'],\n",
    "        use_label_encoder=False,\n",
    "        random_state=42\n",
    "    )\n",
    "    clf.fit(X_train_res, y_train_res)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nROC AUC:\", roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))\n",
    "\n",
    "    mlflow.set_experiment(\"modelo_candidato_sucesso\")\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(clf.get_params())\n",
    "        mlflow.log_metric(\"acuracia\", clf.score(X_test, y_test))\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))\n",
    "        mlflow.log_metric(\"precision\", precision_score(y_test, y_pred))\n",
    "        mlflow.log_metric(\"recall\", recall_score(y_test, y_pred))\n",
    "        mlflow.log_metric(\"f1_score\", f1_score(y_test, y_pred))\n",
    "\n",
    "        importances = clf.feature_importances_\n",
    "        feature_names = X.columns.tolist()\n",
    "        fi_df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
    "        fi_df.to_csv(\"feature_importances.csv\", index=False)\n",
    "        mlflow.log_artifact(\"feature_importances.csv\")\n",
    "\n",
    "        input_example = X_test.iloc[:1]\n",
    "        signature = infer_signature(X_test, clf.predict(X_test))\n",
    "        mlflow.sklearn.log_model(clf, \"modelo_xgboost\", input_example=input_example, signature=signature)\n",
    "\n",
    "    joblib.dump(clf, \"modelo_xgboost.pkl\")\n",
    "    joblib.dump(tfidf, \"vetorizador_tfidf.pkl\")\n",
    "\n",
    "    return clf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"C:\\\\Users\\\\ffporto\\\\Desktop\\\\Estudo\\\\FIAP\\\\fase05\\\\data\\\\\"\n",
    "    df = carregar_dados(f\"{path}dataset_processado.parquet\")\n",
    "    df.columns = df.columns.astype(str)\n",
    "    clf = treinar_modelo_supervisionado(df)\n",
    "    df.to_parquet(f\"{path}dataset_clusterizado.parquet\", index=False)\n",
    "    print(\"Modelos treinados e salvos com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "47f8d1a2-e7ee-4632-9066-71711b471893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuição original do treinamento:\n",
      "contratado\n",
      "0    5372\n",
      "1    1700\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribuição após SMOTEENN no treinamento:\n",
      "contratado\n",
      "1    4420\n",
      "0    2640\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Executando cross-validation com xgb.cv...\n",
      "[0]\ttrain-auc:0.85747+0.00484\ttest-auc:0.81771+0.00646\n",
      "[10]\ttrain-auc:0.96054+0.00187\ttest-auc:0.91824+0.00974\n",
      "[20]\ttrain-auc:0.97793+0.00094\ttest-auc:0.93696+0.00776\n",
      "[30]\ttrain-auc:0.98601+0.00049\ttest-auc:0.94709+0.00609\n",
      "[40]\ttrain-auc:0.99017+0.00044\ttest-auc:0.95293+0.00570\n",
      "[50]\ttrain-auc:0.99314+0.00031\ttest-auc:0.95768+0.00504\n",
      "[60]\ttrain-auc:0.99509+0.00017\ttest-auc:0.96092+0.00473\n",
      "[70]\ttrain-auc:0.99663+0.00020\ttest-auc:0.96403+0.00402\n",
      "[80]\ttrain-auc:0.99765+0.00019\ttest-auc:0.96618+0.00396\n",
      "[90]\ttrain-auc:0.99838+0.00018\ttest-auc:0.96785+0.00376\n",
      "[100]\ttrain-auc:0.99883+0.00015\ttest-auc:0.96910+0.00370\n",
      "[110]\ttrain-auc:0.99918+0.00010\ttest-auc:0.97013+0.00355\n",
      "[120]\ttrain-auc:0.99943+0.00011\ttest-auc:0.97104+0.00327\n",
      "[130]\ttrain-auc:0.99958+0.00010\ttest-auc:0.97199+0.00293\n",
      "[140]\ttrain-auc:0.99970+0.00008\ttest-auc:0.97273+0.00284\n",
      "[150]\ttrain-auc:0.99979+0.00006\ttest-auc:0.97334+0.00283\n",
      "[160]\ttrain-auc:0.99986+0.00006\ttest-auc:0.97393+0.00267\n",
      "[170]\ttrain-auc:0.99990+0.00004\ttest-auc:0.97439+0.00229\n",
      "[180]\ttrain-auc:0.99993+0.00003\ttest-auc:0.97483+0.00216\n",
      "[190]\ttrain-auc:0.99995+0.00003\ttest-auc:0.97513+0.00211\n",
      "[200]\ttrain-auc:0.99997+0.00002\ttest-auc:0.97563+0.00209\n",
      "[210]\ttrain-auc:0.99998+0.00001\ttest-auc:0.97587+0.00208\n",
      "[220]\ttrain-auc:0.99998+0.00001\ttest-auc:0.97611+0.00207\n",
      "[230]\ttrain-auc:0.99999+0.00001\ttest-auc:0.97641+0.00208\n",
      "[240]\ttrain-auc:0.99999+0.00000\ttest-auc:0.97655+0.00211\n",
      "[250]\ttrain-auc:1.00000+0.00000\ttest-auc:0.97676+0.00210\n",
      "[260]\ttrain-auc:1.00000+0.00000\ttest-auc:0.97700+0.00201\n",
      "[270]\ttrain-auc:1.00000+0.00000\ttest-auc:0.97710+0.00190\n",
      "[280]\ttrain-auc:1.00000+0.00000\ttest-auc:0.97730+0.00185\n",
      "[290]\ttrain-auc:1.00000+0.00000\ttest-auc:0.97750+0.00172\n",
      "[299]\ttrain-auc:1.00000+0.00000\ttest-auc:0.97757+0.00167\n",
      "\n",
      "Melhor número de rounds: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ffporto\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:01:34] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report (Padrão 0.5):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.83      0.85      2303\n",
      "           1       0.52      0.57      0.54       728\n",
      "\n",
      "    accuracy                           0.77      3031\n",
      "   macro avg       0.69      0.70      0.70      3031\n",
      "weighted avg       0.78      0.77      0.77      3031\n",
      "\n",
      "\n",
      "Confusion Matrix (Padrão 0.5):\n",
      "[[1920  383]\n",
      " [ 313  415]]\n",
      "\n",
      "ROC AUC: 0.7731136644510506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ffporto\\AppData\\Local\\anaconda3\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/07/16 10:01:50 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b0e0cc8d78498fa557e1d17fcc2ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Candidatos Em Andamento com Previsões ---\n",
      "                situacao_candidado  prob_contratado  predicao_contratado\n",
      "40743                     prospect         0.999417                    1\n",
      "40744                     prospect         0.999209                    1\n",
      "31151                     inscrito         0.998589                    1\n",
      "14531                     prospect         0.997992                    1\n",
      "28728                     prospect         0.997972                    1\n",
      "24086                     aprovado         0.997141                    1\n",
      "28388                     prospect         0.997047                    1\n",
      "8590                      prospect         0.997039                    1\n",
      "2723   encaminhado ao requisitante         0.997039                    1\n",
      "8592                      prospect         0.997039                    1\n",
      "\n",
      "Modelos treinados e salvos com sucesso! Previsões para candidatos em andamento geradas.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier, cv, DMatrix\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def fpreproc(dtrain, dtest, param):\n",
    "    label = dtrain.get_label()\n",
    "    ratio = float(np.sum(label == 0)) / np.sum(label == 1)\n",
    "    param['scale_pos_weight'] = ratio\n",
    "    wtrain = dtrain.get_weight()\n",
    "    wtest = dtest.get_weight()\n",
    "    sum_weight = sum(wtrain) + sum(wtest)\n",
    "    if sum(wtrain) > 0:\n",
    "        wtrain *= sum_weight / sum(wtrain)\n",
    "    if sum(wtest) > 0:\n",
    "        wtest *= sum_weight / sum(wtest)\n",
    "    dtrain.set_weight(wtrain)\n",
    "    dtest.set_weight(wtest)\n",
    "    return dtrain, dtest, param\n",
    "\n",
    "def criar_coluna_contratado_refinada(df):\n",
    "    \"\"\"\n",
    "    Refines the 'contratado' column and splits the DataFrame into training and 'in-progress' data.\n",
    "    \"\"\"\n",
    "    contratado_status = [\n",
    "        'contratado pela decision',\n",
    "        'contratado como hunting',\n",
    "        'proposta aceita'\n",
    "    ]\n",
    "    nao_contratado_status = [\n",
    "        'nao aprovado pelo cliente',\n",
    "        'desistiu',\n",
    "        'nao aprovado pelo rh',\n",
    "        'nao aprovado pelo requisitante',\n",
    "        'sem interesse nesta vaga',\n",
    "        'desistiu da contratacao',\n",
    "        'recusado'\n",
    "    ]\n",
    "\n",
    "    df['contratado'] = np.nan\n",
    "\n",
    "    df.loc[df['situacao_candidado'].isin(contratado_status), 'contratado'] = 1\n",
    "    df.loc[df['situacao_candidado'].isin(nao_contratado_status), 'contratado'] = 0\n",
    "\n",
    "    df_treinamento = df.dropna(subset=['contratado']).copy()\n",
    "    df_treinamento['contratado'] = df_treinamento['contratado'].astype(int)\n",
    "\n",
    "    # df_em_andamento now explicitly contains only rows where 'contratado' was NaN,\n",
    "    # and the 'contratado' column is dropped as it's not applicable for prediction\n",
    "    df_em_andamento = df[df['contratado'].isna()].copy()\n",
    "    if 'contratado' in df_em_andamento.columns:\n",
    "        df_em_andamento.drop(columns=['contratado'], inplace=True)\n",
    "\n",
    "    return df_treinamento, df_em_andamento\n",
    "\n",
    "def carregar_dados(path):\n",
    "    \"\"\"\n",
    "    Loads data from a parquet file.\n",
    "    \"\"\"\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def extrair_features_completas(df):\n",
    "    \"\"\"\n",
    "    Extracts complete features from a DataFrame, fitting a new TfidfVectorizer.\n",
    "    This function should only be used for the training data preparation.\n",
    "    \"\"\"\n",
    "    df.columns = df.columns.astype(str)\n",
    "    texto_completo = (\n",
    "        df['cv'].fillna('') + ' ' +\n",
    "        df['objetivo_profissional'].fillna('') + ' ' +\n",
    "        df['titulo_profissional'].fillna('') + ' ' +\n",
    "        df['principais_atividades_vaga'].fillna('')\n",
    "    )\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=100)\n",
    "    X_texto = tfidf.fit_transform(texto_completo)\n",
    "\n",
    "    X_estrut = df.filter(\n",
    "        regex=r'^(tipo_contratacao_|nivel_profissional_|nivel_academico_|nivel_ingles_|nivel_espanhol_|ingles_vaga_|espanhol_vaga_|feature_mesma_cidade$|^match_|^qtd_keywords_cv$|^sim_cv_atividade$)'\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    X_final = pd.concat([pd.DataFrame(X_texto.toarray()), X_estrut.reset_index(drop=True)], axis=1)\n",
    "    # Ensure column names are strings for XGBoost compatibility\n",
    "    X_final.columns = X_final.columns.astype(str)\n",
    "    return X_final, tfidf\n",
    "\n",
    "def extrair_features_para_predicao(df_para_prever, tfidf_model, original_feature_columns):\n",
    "    \"\"\"\n",
    "    Extracts features from a DataFrame for prediction, using a pre-trained TF-IDF model.\n",
    "    Ensures column consistency with training data.\n",
    "    \"\"\"\n",
    "    df_para_prever.columns = df_para_prever.columns.astype(str)\n",
    "    texto_completo = (\n",
    "        df_para_prever['cv'].fillna('') + ' ' +\n",
    "        df_para_prever['objetivo_profissional'].fillna('') + ' ' +\n",
    "        df_para_prever['titulo_profissional'].fillna('') + ' ' +\n",
    "        df_para_prever['principais_atividades_vaga'].fillna('')\n",
    "    )\n",
    "\n",
    "    # Use the tfidf_model (already fitted) to transform the new data\n",
    "    X_texto = tfidf_model.transform(texto_completo)\n",
    "\n",
    "    X_estrut = df_para_prever.filter(\n",
    "        regex=r'^(tipo_contratacao_|nivel_profissional_|nivel_academico_|nivel_ingles_|nivel_espanhol_|ingles_vaga_|espanhol_vaga_|feature_mesma_cidade$|^match_|^qtd_keywords_cv$|^sim_cv_atividade$)'\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    X_final = pd.concat([pd.DataFrame(X_texto.toarray()), X_estrut.reset_index(drop=True)], axis=1)\n",
    "    X_final.columns = X_final.columns.astype(str) # Ensure column names are strings\n",
    "\n",
    "    # Reindex to ensure all columns from training are present, filling missing with 0 (for new TF-IDF features not seen)\n",
    "    # This is important if max_features in TfidfVectorizer leads to different column counts\n",
    "    X_final = X_final.reindex(columns=original_feature_columns, fill_value=0)\n",
    "    return X_final\n",
    "\n",
    "def treinar_modelo_supervisionado(df_treinamento_input):\n",
    "    \"\"\"\n",
    "    Trains the supervised XGBoost model with refined data.\n",
    "    \"\"\"\n",
    "    df_treinamento_input.columns = df_treinamento_input.columns.astype(str)\n",
    "    X, tfidf = extrair_features_completas(df_treinamento_input) # `tfidf` is now returned\n",
    "    X.columns = X.columns.astype(str)\n",
    "    y = df_treinamento_input['contratado']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    print(\"Distribuição original do treinamento:\")\n",
    "    print(y_train.value_counts())\n",
    "\n",
    "    \"\"\" SMOTEENN\n",
    "    Classification Report (Padrão 0.5):\n",
    "                  precision    recall  f1-score   support\n",
    "    \n",
    "               0       0.86      0.83      0.85      2303\n",
    "               1       0.52      0.57      0.54       728\n",
    "    \n",
    "        accuracy                           0.77      3031\n",
    "       macro avg       0.69      0.70      0.70      3031\n",
    "    weighted avg       0.78      0.77      0.77      3031\n",
    "    \n",
    "    \n",
    "    Confusion Matrix (Padrão 0.5):\n",
    "    [[1920  383]\n",
    "     [ 313  415]]\n",
    "    \n",
    "    ROC AUC: 0.7731136644510506\n",
    "    \n",
    "    SMOTETomek\n",
    "    Classification Report (Padrão 0.5):\n",
    "                  precision    recall  f1-score   support\n",
    "    \n",
    "               0       0.84      0.91      0.87      2303\n",
    "               1       0.62      0.45      0.52       728\n",
    "    \n",
    "        accuracy                           0.80      3031\n",
    "       macro avg       0.73      0.68      0.70      3031\n",
    "    weighted avg       0.79      0.80      0.79      3031\n",
    "    \n",
    "    Confusion Matrix (Padrão 0.5):\n",
    "    [[2104  199]\n",
    "     [ 404  324]]\n",
    "    \n",
    "    ROC AUC: 0.7557265248863164\n",
    "\n",
    "    SMOTE\n",
    "    Classification Report (Padrão 0.5):\n",
    "                  precision    recall  f1-score   support\n",
    "    \n",
    "               0       0.84      0.91      0.87      2303\n",
    "               1       0.61      0.43      0.51       728\n",
    "    \n",
    "        accuracy                           0.80      3031\n",
    "       macro avg       0.72      0.67      0.69      3031\n",
    "    weighted avg       0.78      0.80      0.78      3031\n",
    "    \n",
    "    \n",
    "    Confusion Matrix (Padrão 0.5):\n",
    "    [[2102  201]\n",
    "     [ 413  315]]\n",
    "    \n",
    "    ROC AUC: 0.7590657551306705\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply SMOTEENN on the training set\n",
    "    smoteenn = SMOTEENN(random_state=42)\n",
    "    X_train_res, y_train_res = smoteenn.fit_resample(X_train, y_train)\n",
    "\n",
    "    print(\"\\nDistribuição após SMOTEENN no treinamento:\")\n",
    "    print(y_train_res.value_counts())\n",
    "\n",
    "    # Prepare DMatrix for xgb.cv with the resampled data\n",
    "    # IMPORTANT: Do not apply fpreproc's scale_pos_weight if SMOTEENN already balanced the data to 1:1\n",
    "    # If the ratio is very close to 1 after SMOTEENN, scale_pos_weight in fpreproc will be ~1.\n",
    "    # We can pass an empty dict for param to fpreproc if we explicitly don't want scale_pos_weight applied here.\n",
    "    dtrain = DMatrix(X_train_res, label=y_train_res)\n",
    "    dtest = DMatrix(X_test, label=y_test) # X_test and y_test remain unresampled\n",
    "\n",
    "    param = {\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_delta_step': 1,\n",
    "        'nthread': 16,\n",
    "        'subsample': 0.5,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'seed': 42\n",
    "    }\n",
    "    num_round = 300 # Max rounds for CV\n",
    "\n",
    "    print(\"\\nExecutando cross-validation com xgb.cv...\")\n",
    "    cv_results = cv(\n",
    "        param,\n",
    "        dtrain,\n",
    "        num_boost_round=num_round,\n",
    "        nfold=5,\n",
    "        seed=42,\n",
    "        metrics=['auc'],\n",
    "        fpreproc=fpreproc, # Comment out or remove if SMOTEENN fully balances for CV\n",
    "        # If SMOTEENN creates close to 1:1 ratio, scale_pos_weight will be near 1, effectively doing nothing.\n",
    "        # Keeping it might still be fine, but often omitted if explicit resampling is done.\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=10\n",
    "    )\n",
    "\n",
    "    best_num_round = len(cv_results) if len(cv_results) > 0 else num_round # Fallback in case early stopping doesn't trigger\n",
    "    print(f\"\\nMelhor número de rounds: {best_num_round}\")\n",
    "\n",
    "    clf = XGBClassifier(\n",
    "        max_depth=param['max_depth'],\n",
    "        learning_rate=param['learning_rate'],\n",
    "        max_delta_step=param['max_delta_step'],\n",
    "        n_estimators=best_num_round,\n",
    "        nthread=param['nthread'],\n",
    "        subsample=param['subsample'],\n",
    "        colsample_bytree=param['colsample_bytree'],\n",
    "        # If SMOTEENN fully balances, scale_pos_weight should be 1 or omitted.\n",
    "        # float(np.sum(y_train_res == 0)) / np.sum(y_train_res == 1) will be 1 if perfectly balanced.\n",
    "        scale_pos_weight=float(np.sum(y_train_res == 0)) / np.sum(y_train_res == 1), # Recalculate for resampled data\n",
    "        objective=param['objective'],\n",
    "        eval_metric=param['eval_metric'],\n",
    "        use_label_encoder=False,\n",
    "        random_state=42\n",
    "    )\n",
    "    # Fit the model on the RESAMPLED training data!\n",
    "    clf.fit(X_train_res, y_train_res) # <-- THIS WAS THE KEY CHANGE HERE\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(\"\\nClassification Report (Padrão 0.5):\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix (Padrão 0.5):\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nROC AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "\n",
    "    mlflow.set_experiment(\"modelo_candidato_sucesso\")\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(clf.get_params())\n",
    "        mlflow.log_metric(\"acuracia\", clf.score(X_test, y_test))\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))\n",
    "        mlflow.log_metric(\"precision_class1\", precision_score(y_test, y_pred, pos_label=1))\n",
    "        mlflow.log_metric(\"recall_class1\", recall_score(y_test, y_pred, pos_label=1))\n",
    "        mlflow.log_metric(\"f1_score_class1\", f1_score(y_test, y_pred, pos_label=1))\n",
    "\n",
    "        importances = clf.feature_importances_\n",
    "        feature_names = X.columns.tolist() # Get feature names from the original X before resampling\n",
    "        fi_df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
    "        fi_df.to_csv(\"feature_importances.csv\", index=False)\n",
    "        mlflow.log_artifact(\"feature_importances.csv\")\n",
    "\n",
    "        input_example = X_test.iloc[:1]\n",
    "        signature = infer_signature(X_test, clf.predict(X_test))\n",
    "        mlflow.sklearn.log_model(clf, \"modelo_xgboost\", input_example=input_example, signature=signature)\n",
    "\n",
    "    joblib.dump(clf, \"modelo_xgboost.pkl\")\n",
    "    joblib.dump(tfidf, \"vetorizador_tfidf.pkl\") # Save the fitted tfidf vectorizer\n",
    "\n",
    "    return clf, tfidf, X.columns.tolist() # Return clf, tfidf, and the list of feature columns\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"C:\\\\Users\\\\ffporto\\\\Desktop\\\\Estudo\\\\FIAP\\\\fase05\\\\data\\\\\"\n",
    "    df = carregar_dados(f\"{path}dataset_processado.parquet\")\n",
    "    df.columns = df.columns.astype(str)\n",
    "\n",
    "    # 1. Prepare training data and \"in-progress\" data\n",
    "    df_treinamento, df_em_andamento = criar_coluna_contratado_refinada(df)\n",
    "    \n",
    "    # 2. Train the model and get the TF-IDF vectorizer and original feature columns\n",
    "    clf, tfidf_model, original_feature_columns = treinar_modelo_supervisionado(df_treinamento)\n",
    "\n",
    "    # 3. Prepare \"in-progress\" data for prediction using the loaded tfidf_model\n",
    "    # This new `extrair_features_para_predicao` function will ensure consistency\n",
    "    X_em_andamento = extrair_features_para_predicao(df_em_andamento, tfidf_model, original_feature_columns)\n",
    "\n",
    "    # 4. Make probability predictions for \"in-progress\" candidates\n",
    "    probabilities_em_andamento = clf.predict_proba(X_em_andamento)[:, 1]\n",
    "\n",
    "    # 5. Add predictions back to the 'df_em_andamento' DataFrame\n",
    "    df_em_andamento['prob_contratado'] = probabilities_em_andamento\n",
    "\n",
    "    # 6. Classify with an adjusted threshold for actionable insights\n",
    "    # Adjust this threshold based on your desired balance of precision and recall for 'contratado'\n",
    "    threshold_predicao = 0.5 # Example: You might want to experiment with this value\n",
    "    df_em_andamento['predicao_contratado'] = (df_em_andamento['prob_contratado'] > threshold_predicao).astype(int)\n",
    "\n",
    "    print(\"\\n--- Candidatos Em Andamento com Previsões ---\")\n",
    "    # Display top 10 candidates with highest probability of being hired\n",
    "    print(df_em_andamento[['situacao_candidado', 'prob_contratado', 'predicao_contratado']].sort_values(by='prob_contratado', ascending=False).head(10))\n",
    "\n",
    "    # You can save this DataFrame with predictions for further analysis\n",
    "    df_em_andamento.to_parquet(f\"{path}dataset_em_andamento_com_predicao.parquet\", index=False)\n",
    "    print(\"\\nModelos treinados e salvos com sucesso! Previsões para candidatos em andamento geradas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380cba10-dfd8-47d7-bbf4-b6922da48fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

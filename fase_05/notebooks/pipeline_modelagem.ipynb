{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b477aaad-3eda-41cc-9d7e-9a609c64e80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.2-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\ffporto\\appdata\\local\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\ffporto\\appdata\\local\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-3.0.2-py3-none-win_amd64.whl (150.0 MB)\n",
      "   ---------------------------------------- 0.0/150.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.9/150.0 MB 18.1 MB/s eta 0:00:09\n",
      "   ---------------------------------------- 0.9/150.0 MB 18.1 MB/s eta 0:00:09\n",
      "   ---------------------------------------- 1.1/150.0 MB 9.7 MB/s eta 0:00:16\n",
      "   ---------------------------------------- 1.1/150.0 MB 9.7 MB/s eta 0:00:16\n",
      "   ---------------------------------------- 1.1/150.0 MB 9.7 MB/s eta 0:00:16\n",
      "   ---------------------------------------- 1.1/150.0 MB 9.7 MB/s eta 0:00:16\n",
      "   ---------------------------------------- 1.2/150.0 MB 3.7 MB/s eta 0:00:41\n",
      "   ---------------------------------------- 1.9/150.0 MB 5.6 MB/s eta 0:00:27\n",
      "   ---------------------------------------- 1.9/150.0 MB 5.6 MB/s eta 0:00:27\n",
      "    --------------------------------------- 3.3/150.0 MB 7.0 MB/s eta 0:00:21\n",
      "    --------------------------------------- 3.3/150.0 MB 7.1 MB/s eta 0:00:21\n",
      "    --------------------------------------- 3.3/150.0 MB 7.1 MB/s eta 0:00:21\n",
      "    --------------------------------------- 3.6/150.0 MB 6.4 MB/s eta 0:00:23\n",
      "    --------------------------------------- 3.6/150.0 MB 6.4 MB/s eta 0:00:23\n",
      "    --------------------------------------- 3.6/150.0 MB 6.4 MB/s eta 0:00:23\n",
      "    --------------------------------------- 3.6/150.0 MB 6.4 MB/s eta 0:00:23\n",
      "    --------------------------------------- 3.6/150.0 MB 6.4 MB/s eta 0:00:23\n",
      "    --------------------------------------- 3.7/150.0 MB 4.6 MB/s eta 0:00:32\n",
      "    --------------------------------------- 3.7/150.0 MB 4.6 MB/s eta 0:00:32\n",
      "    --------------------------------------- 3.7/150.0 MB 4.6 MB/s eta 0:00:32\n",
      "    --------------------------------------- 3.7/150.0 MB 4.6 MB/s eta 0:00:32\n",
      "    --------------------------------------- 3.7/150.0 MB 4.6 MB/s eta 0:00:32\n",
      "    --------------------------------------- 3.7/150.0 MB 4.6 MB/s eta 0:00:32\n",
      "    --------------------------------------- 3.7/150.0 MB 4.6 MB/s eta 0:00:32\n",
      "    --------------------------------------- 3.7/150.0 MB 4.6 MB/s eta 0:00:32\n",
      "   - -------------------------------------- 4.0/150.0 MB 3.4 MB/s eta 0:00:44\n",
      "   - -------------------------------------- 4.0/150.0 MB 3.4 MB/s eta 0:00:44\n",
      "   - -------------------------------------- 4.3/150.0 MB 3.3 MB/s eta 0:00:45\n",
      "   - -------------------------------------- 4.3/150.0 MB 3.3 MB/s eta 0:00:45\n",
      "   - -------------------------------------- 4.6/150.0 MB 3.3 MB/s eta 0:00:45\n",
      "   - -------------------------------------- 4.6/150.0 MB 3.3 MB/s eta 0:00:45\n",
      "   - -------------------------------------- 4.6/150.0 MB 3.3 MB/s eta 0:00:45\n",
      "   - -------------------------------------- 4.6/150.0 MB 3.3 MB/s eta 0:00:45\n",
      "   - -------------------------------------- 4.6/150.0 MB 3.3 MB/s eta 0:00:45\n",
      "   - -------------------------------------- 4.6/150.0 MB 2.8 MB/s eta 0:00:52\n",
      "   - -------------------------------------- 4.6/150.0 MB 2.8 MB/s eta 0:00:52\n",
      "   - -------------------------------------- 4.6/150.0 MB 2.8 MB/s eta 0:00:52\n",
      "   - -------------------------------------- 4.9/150.0 MB 2.8 MB/s eta 0:00:53\n",
      "   - -------------------------------------- 4.9/150.0 MB 2.8 MB/s eta 0:00:53\n",
      "   - -------------------------------------- 4.9/150.0 MB 2.8 MB/s eta 0:00:53\n",
      "   - -------------------------------------- 5.6/150.0 MB 3.0 MB/s eta 0:00:49\n",
      "   - -------------------------------------- 5.6/150.0 MB 3.0 MB/s eta 0:00:49\n",
      "   - -------------------------------------- 5.6/150.0 MB 3.0 MB/s eta 0:00:49\n",
      "   - -------------------------------------- 5.6/150.0 MB 3.0 MB/s eta 0:00:49\n",
      "   - -------------------------------------- 5.6/150.0 MB 3.0 MB/s eta 0:00:49\n",
      "   - -------------------------------------- 5.9/150.0 MB 2.8 MB/s eta 0:00:53\n",
      "   - -------------------------------------- 5.9/150.0 MB 2.8 MB/s eta 0:00:53\n",
      "   - -------------------------------------- 5.9/150.0 MB 2.8 MB/s eta 0:00:53\n",
      "   - -------------------------------------- 5.9/150.0 MB 2.6 MB/s eta 0:00:55\n",
      "   - -------------------------------------- 5.9/150.0 MB 2.6 MB/s eta 0:00:55\n",
      "   - -------------------------------------- 5.9/150.0 MB 2.6 MB/s eta 0:00:55\n",
      "   - -------------------------------------- 5.9/150.0 MB 2.6 MB/s eta 0:00:55\n",
      "   - -------------------------------------- 6.2/150.0 MB 2.5 MB/s eta 0:00:58\n",
      "   - -------------------------------------- 6.2/150.0 MB 2.5 MB/s eta 0:00:58\n",
      "   - -------------------------------------- 6.2/150.0 MB 2.5 MB/s eta 0:00:58\n",
      "   - -------------------------------------- 6.5/150.0 MB 2.5 MB/s eta 0:00:57\n",
      "   - -------------------------------------- 6.8/150.0 MB 2.6 MB/s eta 0:00:56\n",
      "   - -------------------------------------- 6.8/150.0 MB 2.6 MB/s eta 0:00:56\n",
      "   - -------------------------------------- 6.8/150.0 MB 2.5 MB/s eta 0:00:58\n",
      "   - -------------------------------------- 6.8/150.0 MB 2.5 MB/s eta 0:00:58\n",
      "   - -------------------------------------- 6.8/150.0 MB 2.5 MB/s eta 0:00:58\n",
      "   - -------------------------------------- 6.8/150.0 MB 2.5 MB/s eta 0:00:58\n",
      "   - -------------------------------------- 6.8/150.0 MB 2.5 MB/s eta 0:00:58\n",
      "   - -------------------------------------- 7.1/150.0 MB 2.4 MB/s eta 0:01:00\n",
      "   - -------------------------------------- 7.1/150.0 MB 2.4 MB/s eta 0:01:00\n",
      "   - -------------------------------------- 7.1/150.0 MB 2.4 MB/s eta 0:01:00\n",
      "   - -------------------------------------- 7.1/150.0 MB 2.4 MB/s eta 0:01:00\n",
      "   - -------------------------------------- 7.1/150.0 MB 2.4 MB/s eta 0:01:00\n",
      "   - -------------------------------------- 7.1/150.0 MB 2.2 MB/s eta 0:01:05\n",
      "   - -------------------------------------- 7.1/150.0 MB 2.2 MB/s eta 0:01:05\n",
      "   - -------------------------------------- 7.1/150.0 MB 2.2 MB/s eta 0:01:05\n",
      "   - -------------------------------------- 7.1/150.0 MB 2.2 MB/s eta 0:01:05\n",
      "   - -------------------------------------- 7.1/150.0 MB 2.2 MB/s eta 0:01:05\n",
      "   - -------------------------------------- 7.1/150.0 MB 2.2 MB/s eta 0:01:05\n",
      "   - -------------------------------------- 7.4/150.0 MB 2.1 MB/s eta 0:01:07\n",
      "   - -------------------------------------- 7.4/150.0 MB 2.1 MB/s eta 0:01:07\n",
      "   - -------------------------------------- 7.4/150.0 MB 2.1 MB/s eta 0:01:09\n",
      "   -- ------------------------------------- 7.7/150.0 MB 2.1 MB/s eta 0:01:07\n",
      "   -- ------------------------------------- 8.0/150.0 MB 2.2 MB/s eta 0:01:05\n",
      "   -- ------------------------------------- 8.0/150.0 MB 2.2 MB/s eta 0:01:05\n",
      "   -- ------------------------------------- 8.3/150.0 MB 2.2 MB/s eta 0:01:04\n",
      "   -- ------------------------------------- 8.3/150.0 MB 2.2 MB/s eta 0:01:04\n",
      "   -- ------------------------------------- 8.3/150.0 MB 2.2 MB/s eta 0:01:04\n",
      "   -- ------------------------------------- 8.3/150.0 MB 2.1 MB/s eta 0:01:07\n",
      "   -- ------------------------------------- 8.6/150.0 MB 2.2 MB/s eta 0:01:06\n",
      "   -- ------------------------------------- 8.6/150.0 MB 2.2 MB/s eta 0:01:06\n",
      "   -- ------------------------------------- 8.6/150.0 MB 2.1 MB/s eta 0:01:07\n",
      "   -- ------------------------------------- 9.2/150.0 MB 2.2 MB/s eta 0:01:03\n",
      "   -- ------------------------------------- 9.2/150.0 MB 2.2 MB/s eta 0:01:03\n",
      "   -- ------------------------------------- 9.5/150.0 MB 2.3 MB/s eta 0:01:02\n",
      "   -- ------------------------------------- 9.8/150.0 MB 2.3 MB/s eta 0:01:01\n",
      "   -- ------------------------------------- 9.8/150.0 MB 2.3 MB/s eta 0:01:02\n",
      "   -- ------------------------------------- 10.7/150.0 MB 2.4 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 11.0/150.0 MB 2.4 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 11.0/150.0 MB 2.4 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 11.0/150.0 MB 2.4 MB/s eta 0:00:59\n",
      "   --- ------------------------------------ 11.8/150.0 MB 2.4 MB/s eta 0:00:57\n",
      "   --- ------------------------------------ 12.8/150.0 MB 2.5 MB/s eta 0:00:56\n",
      "   --- ------------------------------------ 12.9/150.0 MB 2.5 MB/s eta 0:00:56\n",
      "   --- ------------------------------------ 13.3/150.0 MB 2.4 MB/s eta 0:00:57\n",
      "   --- ------------------------------------ 13.7/150.0 MB 2.5 MB/s eta 0:00:55\n",
      "   --- ------------------------------------ 13.8/150.0 MB 2.5 MB/s eta 0:00:56\n",
      "   --- ------------------------------------ 13.8/150.0 MB 2.5 MB/s eta 0:00:56\n",
      "   --- ------------------------------------ 13.8/150.0 MB 2.5 MB/s eta 0:00:56\n",
      "   --- ------------------------------------ 14.0/150.0 MB 2.8 MB/s eta 0:00:50\n",
      "   --- ------------------------------------ 14.0/150.0 MB 2.8 MB/s eta 0:00:50\n",
      "   --- ------------------------------------ 14.1/150.0 MB 2.7 MB/s eta 0:00:51\n",
      "   --- ------------------------------------ 14.1/150.0 MB 2.7 MB/s eta 0:00:51\n",
      "   --- ------------------------------------ 14.1/150.0 MB 2.7 MB/s eta 0:00:51\n",
      "   --- ------------------------------------ 14.1/150.0 MB 2.7 MB/s eta 0:00:51\n",
      "   --- ------------------------------------ 14.1/150.0 MB 2.7 MB/s eta 0:00:51\n",
      "   --- ------------------------------------ 14.4/150.0 MB 2.6 MB/s eta 0:00:52\n",
      "   --- ------------------------------------ 14.4/150.0 MB 2.6 MB/s eta 0:00:52\n",
      "   --- ------------------------------------ 14.9/150.0 MB 2.9 MB/s eta 0:00:48\n",
      "   --- ------------------------------------ 14.9/150.0 MB 2.9 MB/s eta 0:00:48\n",
      "   --- ------------------------------------ 14.9/150.0 MB 2.9 MB/s eta 0:00:48\n",
      "   ---- ----------------------------------- 16.2/150.0 MB 3.4 MB/s eta 0:00:40\n",
      "   ---- ----------------------------------- 16.2/150.0 MB 3.3 MB/s eta 0:00:41\n",
      "   ---- ----------------------------------- 16.6/150.0 MB 3.4 MB/s eta 0:00:39\n",
      "   ---- ----------------------------------- 16.6/150.0 MB 3.4 MB/s eta 0:00:39\n",
      "   ---- ----------------------------------- 16.6/150.0 MB 3.4 MB/s eta 0:00:39\n",
      "   ---- ----------------------------------- 16.6/150.0 MB 3.4 MB/s eta 0:00:39\n",
      "   ---- ----------------------------------- 16.6/150.0 MB 3.4 MB/s eta 0:00:39\n",
      "   ---- ----------------------------------- 17.1/150.0 MB 3.6 MB/s eta 0:00:37\n",
      "   ---- ----------------------------------- 17.1/150.0 MB 3.6 MB/s eta 0:00:37\n",
      "   ---- ----------------------------------- 17.1/150.0 MB 3.6 MB/s eta 0:00:37\n",
      "   ---- ----------------------------------- 17.1/150.0 MB 3.6 MB/s eta 0:00:37\n",
      "   ---- ----------------------------------- 17.4/150.0 MB 4.0 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 17.4/150.0 MB 4.0 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 17.4/150.0 MB 4.0 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 17.4/150.0 MB 4.0 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 17.4/150.0 MB 4.0 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 17.4/150.0 MB 4.0 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 17.4/150.0 MB 4.0 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 17.4/150.0 MB 4.0 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 17.4/150.0 MB 4.0 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 17.4/150.0 MB 4.0 MB/s eta 0:00:34\n",
      "   ---- ----------------------------------- 17.7/150.0 MB 3.5 MB/s eta 0:00:38\n",
      "   ---- ----------------------------------- 17.7/150.0 MB 3.5 MB/s eta 0:00:38\n",
      "   ---- ----------------------------------- 17.7/150.0 MB 3.5 MB/s eta 0:00:38\n",
      "   ---- ----------------------------------- 17.7/150.0 MB 3.5 MB/s eta 0:00:38\n",
      "   ---- ----------------------------------- 17.7/150.0 MB 3.5 MB/s eta 0:00:38\n",
      "   ---- ----------------------------------- 17.7/150.0 MB 3.5 MB/s eta 0:00:38\n",
      "   ---- ----------------------------------- 17.9/150.0 MB 3.2 MB/s eta 0:00:42\n",
      "   ---- ----------------------------------- 17.9/150.0 MB 3.2 MB/s eta 0:00:42\n",
      "   ---- ----------------------------------- 18.0/150.0 MB 3.1 MB/s eta 0:00:43\n",
      "   ---- ----------------------------------- 18.6/150.0 MB 3.3 MB/s eta 0:00:40\n",
      "   ----- ---------------------------------- 19.0/150.0 MB 3.4 MB/s eta 0:00:40\n",
      "   ----- ---------------------------------- 19.4/150.0 MB 3.3 MB/s eta 0:00:40\n",
      "   ----- ---------------------------------- 19.6/150.0 MB 3.3 MB/s eta 0:00:40\n",
      "   ----- ---------------------------------- 20.2/150.0 MB 3.4 MB/s eta 0:00:39\n",
      "   ----- ---------------------------------- 20.5/150.0 MB 3.4 MB/s eta 0:00:39\n",
      "   ----- ---------------------------------- 20.7/150.0 MB 3.3 MB/s eta 0:00:39\n",
      "   ----- ---------------------------------- 21.0/150.0 MB 3.3 MB/s eta 0:00:40\n",
      "   ------ --------------------------------- 22.6/150.0 MB 3.4 MB/s eta 0:00:38\n",
      "   ------ --------------------------------- 24.1/150.0 MB 3.8 MB/s eta 0:00:34\n",
      "   ------ --------------------------------- 25.5/150.0 MB 4.7 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 27.9/150.0 MB 12.1 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 30.2/150.0 MB 23.4 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 31.0/150.0 MB 29.7 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 31.8/150.0 MB 32.7 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 32.8/150.0 MB 31.2 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 33.7/150.0 MB 28.4 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 34.7/150.0 MB 27.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 35.7/150.0 MB 27.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 37.2/150.0 MB 26.2 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 38.8/150.0 MB 25.2 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 40.4/150.0 MB 24.2 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 41.5/150.0 MB 25.2 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 42.6/150.0 MB 25.1 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 43.5/150.0 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 44.4/150.0 MB 26.2 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 45.0/150.0 MB 25.1 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 45.3/150.0 MB 24.2 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 45.6/150.0 MB 21.9 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 45.8/150.0 MB 21.1 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 46.0/150.0 MB 18.7 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 46.2/150.0 MB 17.7 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 46.4/150.0 MB 16.4 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 46.9/150.0 MB 16.0 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 47.1/150.0 MB 14.6 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 47.7/150.0 MB 13.9 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 48.5/150.0 MB 13.9 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 49.2/150.0 MB 13.4 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 50.3/150.0 MB 13.1 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 50.9/150.0 MB 12.6 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 51.8/150.0 MB 12.6 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 52.6/150.0 MB 12.4 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 53.2/150.0 MB 12.1 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 54.0/150.0 MB 11.9 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 55.1/150.0 MB 12.1 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 55.8/150.0 MB 13.4 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 56.8/150.0 MB 16.4 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 57.5/150.0 MB 17.7 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 58.3/150.0 MB 17.7 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 59.0/150.0 MB 17.7 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 59.8/150.0 MB 17.2 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 60.7/150.0 MB 17.2 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 61.5/150.0 MB 17.2 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 62.4/150.0 MB 17.7 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 63.3/150.0 MB 18.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 64.4/150.0 MB 18.7 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 65.5/150.0 MB 18.7 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 66.5/150.0 MB 19.3 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 67.6/150.0 MB 19.3 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 68.6/150.0 MB 19.8 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 69.8/150.0 MB 21.1 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 69.9/150.0 MB 21.1 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 69.9/150.0 MB 21.1 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 69.9/150.0 MB 21.1 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 70.0/150.0 MB 15.6 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 72.0/150.0 MB 17.7 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 73.8/150.0 MB 18.7 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 75.9/150.0 MB 19.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 76.9/150.0 MB 19.3 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 77.8/150.0 MB 18.7 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 78.9/150.0 MB 19.3 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 79.8/150.0 MB 19.3 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 80.8/150.0 MB 27.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 81.5/150.0 MB 25.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 82.3/150.0 MB 23.4 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 82.8/150.0 MB 21.8 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 83.4/150.0 MB 20.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 84.5/150.0 MB 20.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 85.1/150.0 MB 19.3 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 85.7/150.0 MB 18.2 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 86.6/150.0 MB 17.7 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 87.6/150.0 MB 18.2 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 88.7/150.0 MB 17.7 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 89.6/150.0 MB 17.7 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 90.5/150.0 MB 17.7 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 91.2/150.0 MB 17.3 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 91.6/150.0 MB 16.8 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 91.9/150.0 MB 16.0 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 92.1/150.0 MB 15.6 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 92.6/150.0 MB 15.2 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 93.5/150.0 MB 16.0 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 94.4/150.0 MB 15.6 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 95.3/150.0 MB 15.2 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 96.2/150.0 MB 16.0 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 96.9/150.0 MB 16.0 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 97.8/150.0 MB 16.0 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 98.4/150.0 MB 15.2 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 99.2/150.0 MB 15.2 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 99.9/150.0 MB 14.9 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 100.6/150.0 MB 14.6 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 101.4/150.0 MB 15.2 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 102.4/150.0 MB 17.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 102.6/150.0 MB 18.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 102.6/150.0 MB 18.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 102.6/150.0 MB 18.2 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 104.5/150.0 MB 15.6 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 107.3/150.0 MB 18.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 108.5/150.0 MB 19.3 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 109.6/150.0 MB 20.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 110.7/150.0 MB 21.1 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 111.9/150.0 MB 21.8 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 112.8/150.0 MB 22.6 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 114.0/150.0 MB 31.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 115.1/150.0 MB 28.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 116.1/150.0 MB 26.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 117.0/150.0 MB 24.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 117.7/150.0 MB 22.6 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 118.3/150.0 MB 21.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 118.8/150.0 MB 21.1 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 119.4/150.0 MB 19.3 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 119.9/150.0 MB 19.3 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 120.6/150.0 MB 18.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 121.7/150.0 MB 18.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 122.5/150.0 MB 17.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 123.5/150.0 MB 17.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 124.3/150.0 MB 17.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 125.0/150.0 MB 16.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 125.5/150.0 MB 16.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 126.2/150.0 MB 15.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 127.2/150.0 MB 15.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 128.3/150.0 MB 16.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 129.5/150.0 MB 18.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 130.7/150.0 MB 19.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 132.0/150.0 MB 19.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 133.1/150.0 MB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 134.3/150.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 135.5/150.0 MB 22.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 136.4/150.0 MB 24.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 137.0/150.0 MB 23.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 137.4/150.0 MB 21.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 138.3/150.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 139.2/150.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 140.3/150.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 141.7/150.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 142.8/150.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 144.0/150.0 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 144.5/150.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 146.1/150.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 146.1/150.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 146.1/150.0 MB 21.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  146.6/150.0 MB 17.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  149.2/150.0 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  149.6/150.0 MB 21.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  150.0/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 150.0/150.0 MB 4.3 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41785409-7bd3-44c8-a959-082b8c289457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuição antes do SMOTE:\n",
      "contratado\n",
      "0    29988\n",
      "1     1578\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97     12852\n",
      "           1       0.47      0.19      0.27       677\n",
      "\n",
      "    accuracy                           0.95     13529\n",
      "   macro avg       0.71      0.59      0.62     13529\n",
      "weighted avg       0.93      0.95      0.94     13529\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[12706   146]\n",
      " [  549   128]]\n",
      "\n",
      "ROC AUC: 0.789143336638775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ffporto\\AppData\\Local\\anaconda3\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/07/15 14:02:24 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5e7a89e2644d45ab51990f0c2875e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelos treinados e salvos com sucesso!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def carregar_dados(path):\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def extrair_features_completas(df):\n",
    "    df.columns = df.columns.astype(str)\n",
    "    # Vetorização textual combinada de CV, objetivo e atividades da vaga\n",
    "    texto_completo = (\n",
    "        df['cv'].fillna('') + ' ' +\n",
    "        df['objetivo_profissional'].fillna('') + ' ' +\n",
    "        df['titulo_profissional'].fillna('') + ' ' +\n",
    "        df['principais_atividades_vaga'].fillna('')\n",
    "    )\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=500)\n",
    "    X_texto = tfidf.fit_transform(texto_completo)\n",
    "\n",
    "    # Selecionar todas as colunas one-hot e numéricas, incluindo as de match\n",
    "    X_estrut = df.filter(\n",
    "        regex=r'^(tipo_contratacao_|nivel_profissional_|nivel_academico_|nivel_ingles_|nivel_espanhol_|ingles_vaga_|espanhol_vaga_|feature_mesma_cidade$|^match_)'\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Concatenar tudo\n",
    "    X_final = pd.concat([pd.DataFrame(X_texto.toarray()), X_estrut.reset_index(drop=True)], axis=1)\n",
    "    return X_final, tfidf\n",
    "\n",
    "def treinar_modelo_supervisionado(df):\n",
    "    df.columns = df.columns.astype(str)\n",
    "    X, tfidf = extrair_features_completas(df)\n",
    "    X.columns = X.columns.astype(str)\n",
    "    y = df['contratado']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    print(\"Distribuição antes do SMOTE:\")\n",
    "    print(y_train.value_counts())\n",
    "    scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "\n",
    "    # smote = SMOTE(random_state=42, sampling_strategy=0.5, k_neighbors=3)\n",
    "    # X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # print(\"\\nDistribuição após SMOTE:\")\n",
    "    # print(y_train_bal.value_counts())\n",
    "\n",
    "    # scale_pos_weight = len(y_train_bal[y_train_bal == 0]) / len(y_train_bal[y_train_bal == 1])\n",
    "    clf = XGBClassifier(\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        max_depth=32,\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=300,\n",
    "        max_delta_step=10,\n",
    "        eta=0.1,\n",
    "        subsample=0.5,\n",
    "        eval_metric='auc',\n",
    "        nthread=16,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        objective='binary:logitraw'\n",
    "    )\n",
    "    # clf.fit(X_train_bal, y_train_bal)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nROC AUC:\", roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))\n",
    "\n",
    "    mlflow.set_experiment(\"modelo_candidato_sucesso\")\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(clf.get_params())\n",
    "        # Métricas adicionais\n",
    "        mlflow.log_metric(\"acuracia\", clf.score(X_test, y_test))\n",
    "        mlflow.log_metric(\"precision\", precision_score(y_test, y_pred))\n",
    "        mlflow.log_metric(\"recall\", recall_score(y_test, y_pred))\n",
    "        mlflow.log_metric(\"f1_score\", f1_score(y_test, y_pred))\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))\n",
    "        \n",
    "        # Feature importance\n",
    "        importances = clf.feature_importances_\n",
    "        feature_names = X.columns.tolist()\n",
    "        fi_df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
    "        fi_df.to_csv(\"feature_importances.csv\", index=False)\n",
    "        mlflow.log_artifact(\"feature_importances.csv\")\n",
    "        \n",
    "        # Input example e assinatura\n",
    "        input_example = X_test.iloc[:1]\n",
    "        signature = infer_signature(X_test, clf.predict(X_test))\n",
    "        mlflow.sklearn.log_model(clf, \"modelo_xgboost\", input_example=input_example, signature=signature)\n",
    "\n",
    "    joblib.dump(clf, \"modelo_xgboost.pkl\")\n",
    "    joblib.dump(tfidf, \"vetorizador_tfidf.pkl\")\n",
    "\n",
    "    return clf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"C:\\\\Users\\\\ffporto\\\\Desktop\\\\Estudo\\\\FIAP\\\\fase05\\\\data\\\\\"\n",
    "    df = carregar_dados(f\"{path}dataset_processado.parquet\")\n",
    "    df.columns = df.columns.astype(str)\n",
    "    clf = treinar_modelo_supervisionado(df)\n",
    "    df.to_parquet(f\"{path}dataset_clusterizado.parquet\", index=False)\n",
    "    print(\"Modelos treinados e salvos com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0282f1f5-9b2e-4e38-83e2-2fa30e12fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score, precision_recall_curve\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier, cv, DMatrix\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def carregar_dados(path):\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def extrair_features_completas(df):\n",
    "    df.columns = df.columns.astype(str)\n",
    "    texto_completo = (\n",
    "        df['cv'].fillna('') + ' ' +\n",
    "        df['objetivo_profissional'].fillna('') + ' ' +\n",
    "        df['titulo_profissional'].fillna('') + ' ' +\n",
    "        df['principais_atividades_vaga'].fillna('')\n",
    "    )\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=100)\n",
    "    X_texto = tfidf.fit_transform(texto_completo)\n",
    "\n",
    "    X_estrut = df.filter(\n",
    "        regex=r'^(tipo_contratacao_|nivel_profissional_|nivel_academico_|nivel_ingles_|nivel_espanhol_|ingles_vaga_|espanhol_vaga_|feature_mesma_cidade$|^match_|^qtd_keywords_cv$|^sim_cv_atividade$)'\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    X_final = pd.concat([pd.DataFrame(X_texto.toarray()), X_estrut.reset_index(drop=True)], axis=1)\n",
    "    return X_final, tfidf\n",
    "\n",
    "def fpreproc(dtrain, dtest, param):\n",
    "    label = dtrain.get_label()\n",
    "    ratio = float(np.sum(label == 0)) / np.sum(label == 1)\n",
    "    param['scale_pos_weight'] = ratio\n",
    "    wtrain = dtrain.get_weight()\n",
    "    wtest = dtest.get_weight()\n",
    "    sum_weight = sum(wtrain) + sum(wtest)\n",
    "    if sum(wtrain) > 0:\n",
    "        wtrain *= sum_weight / sum(wtrain)\n",
    "    if sum(wtest) > 0:\n",
    "        wtest *= sum_weight / sum(wtest)\n",
    "    dtrain.set_weight(wtrain)\n",
    "    dtest.set_weight(wtest)\n",
    "    return dtrain, dtest, param\n",
    "\n",
    "def plot_threshold_curve(y_test, y_probs):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    best_thresh = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(thresholds, f1_scores[:-1], label=\"F1 Score\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.title(\"F1 Score vs Threshold\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"f1_threshold_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Melhor threshold para F1: {best_thresh:.2f}\")\n",
    "    return best_thresh\n",
    "\n",
    "\n",
    "def grid_search_xgboost(X_train, y_train):\n",
    "    param_grid = {\n",
    "        'max_depth': [8, 16, 32],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_delta_step': [5,10,20],\n",
    "        'n_estimators': [150, 250, 350],\n",
    "        'nthread': [8,16,32],\n",
    "        'subsample': [0.5, 0.7, 0.8],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "    }\n",
    "    clf = XGBClassifier(scale_pos_weight=float(np.sum(y_train == 0)) / np.sum(y_train == 1),\n",
    "                        eval_metric='auc',\n",
    "                        objective='binary:logistic',\n",
    "                        use_label_encoder=False,\n",
    "                        random_state=42)\n",
    "    grid = GridSearchCV(clf, param_grid, scoring='roc_auc', cv=3, verbose=1, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(\"Melhores parâmetros do GridSearch:\", grid.best_params_)\n",
    "    return grid.best_estimator_\n",
    "\n",
    "def treinar_modelo_supervisionado(df):\n",
    "    df.columns = df.columns.astype(str)\n",
    "    X, tfidf = extrair_features_completas(df)\n",
    "    X.columns = X.columns.astype(str)\n",
    "    y = df['contratado']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    print(\"Distribuição original:\")\n",
    "    print(y_train.value_counts())\n",
    "\n",
    "    dtrain = DMatrix(X_train, label=y_train)\n",
    "    dtest = DMatrix(X_test, label=y_test)\n",
    "\n",
    "    param = {\n",
    "        'max_depth': 32,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_delta_step': 10,\n",
    "        'n_estimators': 300,\n",
    "        'nthread': 16,\n",
    "        'eta': 0.1,\n",
    "        'subsample': 0.5,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'seed': 42\n",
    "    }\n",
    "    num_round = 300\n",
    "\n",
    "    print(\"\\nExecutando cross-validation com xgb.cv...\")\n",
    "    cv_results = cv(\n",
    "        param,\n",
    "        dtrain,\n",
    "        num_boost_round=num_round,\n",
    "        nfold=5,\n",
    "        seed=42,\n",
    "        metrics=['auc'],\n",
    "        fpreproc=fpreproc,\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=10\n",
    "    )\n",
    "\n",
    "    best_num_round = len(cv_results)\n",
    "    print(f\"\\nMelhor número de rounds: {best_num_round}\")\n",
    "\n",
    "    clf = grid_search_xgboost(X_train, y_train)\n",
    "    clf.set_params(n_estimators=best_num_round)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_probs = clf.predict_proba(X_test)[:, 1]\n",
    "    best_thresh = plot_threshold_curve(y_test, y_probs)\n",
    "    y_pred = (y_probs >= best_thresh).astype(int)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nROC AUC:\", roc_auc_score(y_test, y_probs))\n",
    "\n",
    "    mlflow.set_experiment(\"modelo_candidato_sucesso\")\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(clf.get_params())\n",
    "        mlflow.log_metric(\"acuracia\", clf.score(X_test, y_test))\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc_score(y_test, y_probs))\n",
    "        mlflow.log_metric(\"precision\", precision_score(y_test, y_pred))\n",
    "        mlflow.log_metric(\"recall\", recall_score(y_test, y_pred))\n",
    "        mlflow.log_metric(\"f1_score\", f1_score(y_test, y_pred))\n",
    "        mlflow.log_artifact(\"f1_threshold_plot.png\")\n",
    "\n",
    "        importances = clf.feature_importances_\n",
    "        feature_names = X.columns.tolist()\n",
    "        fi_df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
    "        fi_df.to_csv(\"feature_importances.csv\", index=False)\n",
    "        mlflow.log_artifact(\"feature_importances.csv\")\n",
    "\n",
    "        input_example = X_test.iloc[:1]\n",
    "        signature = infer_signature(X_test, clf.predict(X_test))\n",
    "        mlflow.sklearn.log_model(clf, \"modelo_xgboost\", input_example=input_example, signature=signature)\n",
    "\n",
    "    joblib.dump(clf, \"modelo_xgboost.pkl\")\n",
    "    joblib.dump(tfidf, \"vetorizador_tfidf.pkl\")\n",
    "\n",
    "    return clf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"C:\\\\Users\\\\ffporto\\\\Desktop\\\\Estudo\\\\FIAP\\\\fase05\\\\data\\\\\"\n",
    "    df = carregar_dados(f\"{path}dataset_processado.parquet\")\n",
    "    df.columns = df.columns.astype(str)\n",
    "    clf = treinar_modelo_supervisionado(df)\n",
    "    df.to_parquet(f\"{path}dataset_clusterizado.parquet\", index=False)\n",
    "    print(\"Modelos treinados e salvos com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bcf79f01-8106-42ff-9487-b910d8c8c415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuição original do treinamento:\n",
      "contratado\n",
      "0    29988\n",
      "1     1578\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribuição após SMOTE no treinamento:\n",
      "contratado\n",
      "1    29528\n",
      "0    23742\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Executando cross-validation com xgb.cv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ffporto\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:209: UserWarning: [08:08:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  return getattr(self.bst, name)(*args, **kwargs)\n",
      "C:\\Users\\ffporto\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:215: UserWarning: [08:08:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.bst.update(self.dtrain, iteration, fobj)\n",
      "C:\\Users\\ffporto\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:215: UserWarning: [08:08:26] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.bst.update(self.dtrain, iteration, fobj)\n",
      "C:\\Users\\ffporto\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:215: UserWarning: [08:08:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.bst.update(self.dtrain, iteration, fobj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.86939+0.00292\ttest-auc:0.86127+0.00524\n",
      "[10]\ttrain-auc:0.96273+0.00112\ttest-auc:0.95483+0.00340\n",
      "[20]\ttrain-auc:0.97711+0.00088\ttest-auc:0.97061+0.00238\n",
      "[30]\ttrain-auc:0.98333+0.00034\ttest-auc:0.97743+0.00181\n",
      "[40]\ttrain-auc:0.98662+0.00032\ttest-auc:0.98117+0.00166\n",
      "[50]\ttrain-auc:0.98914+0.00029\ttest-auc:0.98397+0.00144\n",
      "[60]\ttrain-auc:0.99114+0.00019\ttest-auc:0.98623+0.00127\n",
      "[70]\ttrain-auc:0.99275+0.00016\ttest-auc:0.98800+0.00116\n",
      "[80]\ttrain-auc:0.99399+0.00013\ttest-auc:0.98941+0.00108\n",
      "[90]\ttrain-auc:0.99502+0.00009\ttest-auc:0.99065+0.00096\n",
      "[100]\ttrain-auc:0.99577+0.00011\ttest-auc:0.99154+0.00084\n",
      "[110]\ttrain-auc:0.99640+0.00008\ttest-auc:0.99228+0.00077\n",
      "[120]\ttrain-auc:0.99687+0.00008\ttest-auc:0.99283+0.00073\n",
      "[130]\ttrain-auc:0.99731+0.00004\ttest-auc:0.99338+0.00070\n",
      "[140]\ttrain-auc:0.99768+0.00004\ttest-auc:0.99384+0.00069\n",
      "[150]\ttrain-auc:0.99797+0.00004\ttest-auc:0.99421+0.00065\n",
      "[160]\ttrain-auc:0.99821+0.00004\ttest-auc:0.99449+0.00063\n",
      "[170]\ttrain-auc:0.99844+0.00005\ttest-auc:0.99476+0.00062\n",
      "[180]\ttrain-auc:0.99863+0.00005\ttest-auc:0.99501+0.00058\n",
      "[190]\ttrain-auc:0.99878+0.00005\ttest-auc:0.99521+0.00055\n",
      "[200]\ttrain-auc:0.99891+0.00004\ttest-auc:0.99539+0.00052\n",
      "[210]\ttrain-auc:0.99904+0.00005\ttest-auc:0.99556+0.00051\n",
      "[220]\ttrain-auc:0.99914+0.00004\ttest-auc:0.99568+0.00049\n",
      "[230]\ttrain-auc:0.99923+0.00004\ttest-auc:0.99582+0.00049\n",
      "[240]\ttrain-auc:0.99931+0.00004\ttest-auc:0.99595+0.00047\n",
      "[250]\ttrain-auc:0.99939+0.00004\ttest-auc:0.99609+0.00047\n",
      "[260]\ttrain-auc:0.99946+0.00003\ttest-auc:0.99620+0.00046\n",
      "[270]\ttrain-auc:0.99952+0.00003\ttest-auc:0.99630+0.00046\n",
      "[280]\ttrain-auc:0.99958+0.00003\ttest-auc:0.99638+0.00046\n",
      "[290]\ttrain-auc:0.99963+0.00002\ttest-auc:0.99646+0.00047\n",
      "[299]\ttrain-auc:0.99966+0.00002\ttest-auc:0.99651+0.00047\n",
      "\n",
      "Melhor número de rounds: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ffporto\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:10:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96     12852\n",
      "           1       0.29      0.40      0.34       677\n",
      "\n",
      "    accuracy                           0.92     13529\n",
      "   macro avg       0.63      0.67      0.65     13529\n",
      "weighted avg       0.93      0.92      0.93     13529\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[12191   661]\n",
      " [  406   271]]\n",
      "\n",
      "ROC AUC: 0.8012708365801597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ffporto\\AppData\\Local\\anaconda3\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/07/16 08:11:21 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3ffdc2f3af47c482ff877fb51cd3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelos treinados e salvos com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier, cv, DMatrix\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def carregar_dados(path):\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def extrair_features_completas(df):\n",
    "    df.columns = df.columns.astype(str)\n",
    "    texto_completo = (\n",
    "        df['cv'].fillna('') + ' ' +\n",
    "        df['objetivo_profissional'].fillna('') + ' ' +\n",
    "        df['titulo_profissional'].fillna('') + ' ' +\n",
    "        df['principais_atividades_vaga'].fillna('')\n",
    "    )\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=100)\n",
    "    X_texto = tfidf.fit_transform(texto_completo)\n",
    "\n",
    "    X_estrut = df.filter(\n",
    "       regex=r'^(tipo_contratacao_|nivel_profissional_|nivel_academico_|nivel_ingles_|nivel_espanhol_|ingles_vaga_|espanhol_vaga_|feature_mesma_cidade$|^match_|^qtd_keywords_cv$|^sim_cv_atividade$)'\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    X_final = pd.concat([pd.DataFrame(X_texto.toarray()), X_estrut.reset_index(drop=True)], axis=1)\n",
    "    return X_final, tfidf\n",
    "\n",
    "def fpreproc(dtrain, dtest, param):\n",
    "    label = dtrain.get_label()\n",
    "    ratio = float(np.sum(label == 0)) / np.sum(label == 1)\n",
    "    param['scale_pos_weight'] = ratio\n",
    "    wtrain = dtrain.get_weight()\n",
    "    wtest = dtest.get_weight()\n",
    "    sum_weight = sum(wtrain) + sum(wtest)\n",
    "    if sum(wtrain) > 0:\n",
    "        wtrain *= sum_weight / sum(wtrain)\n",
    "    if sum(wtest) > 0:\n",
    "        wtest *= sum_weight / sum(wtest)\n",
    "    dtrain.set_weight(wtrain)\n",
    "    dtest.set_weight(wtest)\n",
    "    return dtrain, dtest, param\n",
    "\n",
    "def treinar_modelo_supervisionado(df):\n",
    "    df.columns = df.columns.astype(str)\n",
    "    X, tfidf = extrair_features_completas(df)\n",
    "    X.columns = X.columns.astype(str)\n",
    "    y = df['contratado']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    print(\"Distribuição original do treinamento:\")\n",
    "    print(y_train.value_counts())\n",
    "\n",
    "    # Aplicar SMOTE APENAS no conjunto de treinamento\n",
    "    smoteenn = SMOTEENN(random_state=42)\n",
    "    X_train_res, y_train_res = smoteenn.fit_resample(X_train, y_train)\n",
    "\n",
    "    print(\"\\nDistribuição após SMOTE no treinamento:\")\n",
    "    print(y_train_res.value_counts())\n",
    "\n",
    "    dtrain = DMatrix(X_train_res, label=y_train_res)\n",
    "    dtest = DMatrix(X_test, label=y_test) # O dtest não deve ser reamostrado\n",
    "    \n",
    "    param = {\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_delta_step': 1,\n",
    "        'n_estimators': 300,\n",
    "        'nthread': 16,\n",
    "        # 'eta': 0.1,\n",
    "        'subsample': 0.5,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'seed': 42\n",
    "    }\n",
    "    num_round = param['n_estimators']\n",
    "\n",
    "    print(\"\\nExecutando cross-validation com xgb.cv...\")\n",
    "    cv_results = cv(\n",
    "        param,\n",
    "        dtrain,\n",
    "        num_boost_round=num_round,\n",
    "        nfold=5,\n",
    "        seed=42,\n",
    "        metrics=['auc'],\n",
    "        fpreproc=fpreproc,\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=10\n",
    "    )\n",
    "\n",
    "    best_num_round = len(cv_results)\n",
    "    print(f\"\\nMelhor número de rounds: {best_num_round}\")\n",
    "\n",
    "    clf = XGBClassifier(\n",
    "        max_depth=param['max_depth'],\n",
    "        learning_rate=param['learning_rate'],\n",
    "        max_delta_step=param['max_delta_step'],\n",
    "        n_estimators=best_num_round,\n",
    "        nthread=param['nthread'],\n",
    "        # eta=param['eta'],\n",
    "        subsample=param['subsample'],\n",
    "        colsample_bytree=param['colsample_bytree'],\n",
    "        # scale_pos_weight=float(np.sum(y_train == 0)) / np.sum(y_train == 1),\n",
    "        objective=param['objective'],\n",
    "        eval_metric=param['eval_metric'],\n",
    "        use_label_encoder=False,\n",
    "        random_state=42\n",
    "    )\n",
    "    clf.fit(X_train_res, y_train_res)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nROC AUC:\", roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))\n",
    "\n",
    "    mlflow.set_experiment(\"modelo_candidato_sucesso\")\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(clf.get_params())\n",
    "        mlflow.log_metric(\"acuracia\", clf.score(X_test, y_test))\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))\n",
    "        mlflow.log_metric(\"precision\", precision_score(y_test, y_pred))\n",
    "        mlflow.log_metric(\"recall\", recall_score(y_test, y_pred))\n",
    "        mlflow.log_metric(\"f1_score\", f1_score(y_test, y_pred))\n",
    "\n",
    "        importances = clf.feature_importances_\n",
    "        feature_names = X.columns.tolist()\n",
    "        fi_df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
    "        fi_df.to_csv(\"feature_importances.csv\", index=False)\n",
    "        mlflow.log_artifact(\"feature_importances.csv\")\n",
    "\n",
    "        input_example = X_test.iloc[:1]\n",
    "        signature = infer_signature(X_test, clf.predict(X_test))\n",
    "        mlflow.sklearn.log_model(clf, \"modelo_xgboost\", input_example=input_example, signature=signature)\n",
    "\n",
    "    joblib.dump(clf, \"modelo_xgboost.pkl\")\n",
    "    joblib.dump(tfidf, \"vetorizador_tfidf.pkl\")\n",
    "\n",
    "    return clf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"C:\\\\Users\\\\ffporto\\\\Desktop\\\\Estudo\\\\FIAP\\\\fase05\\\\data\\\\\"\n",
    "    df = carregar_dados(f\"{path}dataset_processado.parquet\")\n",
    "    df.columns = df.columns.astype(str)\n",
    "    clf = treinar_modelo_supervisionado(df)\n",
    "    df.to_parquet(f\"{path}dataset_clusterizado.parquet\", index=False)\n",
    "    print(\"Modelos treinados e salvos com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "47f8d1a2-e7ee-4632-9066-71711b471893",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'preprocess'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpreprocess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pre_processar \u001b[38;5;66;03m# Import the pre_processar function\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfpreproc\u001b[39m(dtrain, dtest, param):\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    Pré-processamento dos dados de treino e teste antes da validação cruzada do XGBoost.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m        com pesos atualizados, e o dicionário de parâmetros com `scale_pos_weight` ajustado.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'preprocess'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder # Import OneHotEncoder\n",
    "from xgboost import XGBClassifier, cv, DMatrix\n",
    "from imblearn.combine import SMOTEENN\n",
    "from mlflow.models.signature import infer_signature\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from preprocess import pre_processar # Import the pre_processar function\n",
    "\n",
    "\n",
    "def fpreproc(dtrain, dtest, param):\n",
    "    \"\"\"\n",
    "    Pré-processamento dos dados de treino e teste antes da validação cruzada do XGBoost.\n",
    "\n",
    "    Esta função ajusta dinamicamente o parâmetro `scale_pos_weight` com base na proporção\n",
    "    entre classes majoritária e minoritária. Além disso, ela reescala os pesos dos conjuntos\n",
    "    de treino e teste para garantir que o impacto das instâncias seja proporcional e\n",
    "    comparável durante o processo de validação cruzada.\n",
    "\n",
    "    Args:\n",
    "        dtrain (xgboost.DMatrix): Conjunto de dados de treino com rótulos e pesos.\n",
    "        dtest (xgboost.DMatrix): Conjunto de dados de teste com rótulos e pesos.\n",
    "        param (dict): Dicionário de parâmetros do modelo XGBoost.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[xgboost.DMatrix, xgboost.DMatrix, dict]: Os objetos `dtrain` e `dtest`\n",
    "        com pesos atualizados, e o dicionário de parâmetros com `scale_pos_weight` ajustado.\n",
    "    \"\"\"\n",
    "    label = dtrain.get_label()\n",
    "    # Check if there are instances of the minority class to avoid division by zero\n",
    "    if np.sum(label == 1) > 0:\n",
    "        ratio = float(np.sum(label == 0)) / np.sum(label == 1)\n",
    "        param['scale_pos_weight'] = ratio\n",
    "    else:\n",
    "        # If no positive samples, scale_pos_weight might not be applicable or set to a default\n",
    "        param['scale_pos_weight'] = 1.0 # Or handle as an error\n",
    "        print(\"Warning: No positive samples found in dtrain for scale_pos_weight calculation.\")\n",
    "\n",
    "    wtrain = dtrain.get_weight()\n",
    "    wtest = dtest.get_weight()\n",
    "\n",
    "    # Only re-scale weights if they exist and sum > 0 to avoid division by zero\n",
    "    if wtrain is not None and sum(wtrain) > 0:\n",
    "        sum_weight_train = sum(wtrain)\n",
    "    else:\n",
    "        sum_weight_train = 0\n",
    "\n",
    "    if wtest is not None and sum(wtest) > 0:\n",
    "        sum_weight_test = sum(wtest)\n",
    "    else:\n",
    "        sum_weight_test = 0\n",
    "\n",
    "    total_sum_weight = sum_weight_train + sum_weight_test\n",
    "\n",
    "    if total_sum_weight > 0:\n",
    "        if sum_weight_train > 0:\n",
    "            wtrain *= total_sum_weight / sum_weight_train\n",
    "        if sum_weight_test > 0:\n",
    "            wtest *= total_sum_weight / sum_weight_test\n",
    "\n",
    "        if wtrain is not None:\n",
    "            dtrain.set_weight(wtrain)\n",
    "        if wtest is not None:\n",
    "            dtest.set_weight(wtest)\n",
    "    return dtrain, dtest, param\n",
    "\n",
    "\n",
    "def criar_coluna_contratado_refinada(df):\n",
    "    \"\"\"\n",
    "    Refina a coluna 'contratado' com base na situação do candidato e separa o dataset\n",
    "    entre dados de treinamento (com rótulo definido) e dados em andamento (sem rótulo).\n",
    "\n",
    "    Define `contratado = 1` para situações claramente bem-sucedidas no processo seletivo,\n",
    "    `contratado = 0` para rejeições ou desistências, e mantém como NaN os casos indefinidos\n",
    "    ou em andamento. Após o processamento, separa o DataFrame original em dois subconjuntos:\n",
    "    um para treinamento supervisionado e outro com candidatos ainda em processo.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame contendo, entre outras, a coluna 'situacao_candidado'.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "            - df_treinamento: Subconjunto com a coluna 'contratado' preenchida (0 ou 1), pronto para treinamento.\n",
    "            - df_em_andamento: Subconjunto com candidatos sem definição final, sem a coluna 'contratado'.\n",
    "    \"\"\"\n",
    "    contratado_status = [\n",
    "        'contratado pela decision',\n",
    "        'contratado como hunting',\n",
    "        'proposta aceita'\n",
    "    ]\n",
    "    nao_contratado_status = [\n",
    "        'nao aprovado pelo cliente',\n",
    "        'desistiu',\n",
    "        'nao aprovado pelo rh',\n",
    "        'nao aprovado pelo requisitante',\n",
    "        'sem interesse nesta vaga',\n",
    "        'desistiu da contratacao',\n",
    "        'recusado'\n",
    "    ]\n",
    "\n",
    "    df['contratado'] = np.nan\n",
    "\n",
    "    df.loc[df['situacao_candidado'].isin(contratado_status), 'contratado'] = 1\n",
    "    df.loc[df['situacao_candidado'].isin(nao_contratado_status), 'contratado'] = 0\n",
    "\n",
    "    df_treinamento = df.dropna(subset=['contratado']).copy()\n",
    "    df_treinamento['contratado'] = df_treinamento['contratado'].astype(int)\n",
    "\n",
    "    # df_em_andamento now explicitly contains only rows where 'contratado' was NaN,\n",
    "    # and the 'contratado' column is dropped as it's not applicable for prediction\n",
    "    df_em_andamento = df[df['contratado'].isna()].copy()\n",
    "    if 'contratado' in df_em_andamento.columns:\n",
    "        df_em_andamento.drop(columns=['contratado'], inplace=True)\n",
    "\n",
    "    return df_treinamento, df_em_andamento\n",
    "\n",
    "\n",
    "def carregar_dados(path):\n",
    "    \"\"\"\n",
    "    Carrega dados a partir de um arquivo no formato Parquet.\n",
    "\n",
    "    Args:\n",
    "        path (str): Caminho completo para o arquivo .parquet.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame contendo os dados carregados do arquivo.\n",
    "    \"\"\"\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "\n",
    "def extrair_e_transformar_features(df_input, tfidf_model=None, ohe_models=None, original_feature_columns=None, is_training=True):\n",
    "    \"\"\"\n",
    "    Extrai e transforma um conjunto completo de features a partir de dados textuais e estruturados,\n",
    "    aplicando vetorização TF-IDF e One-Hot Encoding.\n",
    "\n",
    "    Args:\n",
    "        df_input (pd.DataFrame): DataFrame contendo as colunas de texto e estruturadas.\n",
    "        tfidf_model (TfidfVectorizer, opcional): Modelo TF-IDF previamente ajustado.\n",
    "            Se None e is_training for True, um novo modelo será ajustado.\n",
    "        ohe_models (dict, opcional): Dicionário de objetos OneHotEncoder previamente ajustados.\n",
    "            As chaves são os nomes das colunas e os valores são os objetos OHE.\n",
    "            Se None e is_training for True, novos modelos serão ajustados.\n",
    "        original_feature_columns (list, opcional): Lista de nomes das colunas de features esperadas\n",
    "            para garantir consistência na ordem e presença das colunas.\n",
    "        is_training (bool): Indica se a função está sendo chamada para o conjunto de treino (True)\n",
    "            ou para predição/teste (False).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, TfidfVectorizer, dict, list]:\n",
    "            - Um DataFrame com as features combinadas (TF-IDF + One-Hot + numéricas/binárias).\n",
    "            - O objeto `TfidfVectorizer` (ajustado ou passado).\n",
    "            - O dicionário de objetos `OneHotEncoder` (ajustados ou passados).\n",
    "            - A lista de nomes das colunas finais de features.\n",
    "    \"\"\"\n",
    "    df = df_input.copy()\n",
    "    df.columns = df.columns.astype(str)\n",
    "\n",
    "    # 1. Pré-processamento de texto e features diretas (comum a treino e inferência)\n",
    "    # A função pre_processar agora faz isso e não mais o One-Hot Encoding.\n",
    "    df = pre_processar(df)\n",
    "\n",
    "\n",
    "    texto_completo = (\n",
    "            df['cv'].fillna('') + ' ' +\n",
    "            df['objetivo_profissional'].fillna('') + ' ' +\n",
    "            df['titulo_profissional'].fillna('') + ' ' +\n",
    "            df['principais_atividades_vaga'].fillna('')\n",
    "    )\n",
    "\n",
    "    # TF-IDF\n",
    "    if is_training:\n",
    "        tfidf = TfidfVectorizer(max_features=100)\n",
    "        X_texto = tfidf.fit_transform(texto_completo)\n",
    "    else:\n",
    "        if tfidf_model is None:\n",
    "            raise ValueError(\"tfidf_model must be provided for prediction/test.\")\n",
    "        tfidf = tfidf_model\n",
    "        X_texto = tfidf.transform(texto_completo)\n",
    "\n",
    "    X_texto_df = pd.DataFrame(X_texto.toarray())\n",
    "    # Ensure TF-IDF column names are strings\n",
    "    X_texto_df.columns = [f'tfidf_{i}' for i in range(X_texto_df.shape[1])]\n",
    "\n",
    "\n",
    "    # One-Hot Encoding\n",
    "    cols_to_encode = [\n",
    "        \"tipo_contratacao\", \"nivel_profissional\", \"nivel_academico\",\n",
    "        \"nivel_ingles\", \"nivel_espanhol\", \"ingles_vaga\", \"espanhol_vaga\",\n",
    "        \"nivel_academico_vaga\"\n",
    "    ]\n",
    "\n",
    "    ohe_fitted_models = {}\n",
    "    df_encoded_features = pd.DataFrame(index=df.index) # Initialize with original index\n",
    "\n",
    "    for col in cols_to_encode:\n",
    "        if is_training:\n",
    "            ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "            encoded_data = ohe.fit_transform(df[[col]])\n",
    "            ohe_fitted_models[col] = ohe # Save the fitted OHE model\n",
    "        else:\n",
    "            if ohe_models is None or col not in ohe_models:\n",
    "                raise ValueError(f\"OneHotEncoder for column '{col}' must be provided for prediction/test.\")\n",
    "            ohe = ohe_models[col]\n",
    "            encoded_data = ohe.transform(df[[col]])\n",
    "\n",
    "        new_cols_names = ohe.get_feature_names_out([col])\n",
    "        temp_df = pd.DataFrame(encoded_data, columns=new_cols_names, index=df.index)\n",
    "        df_encoded_features = pd.concat([df_encoded_features, temp_df], axis=1)\n",
    "\n",
    "    # Numeric and binary features (already processed by pre_processar)\n",
    "    X_numeric_binary = df.filter(\n",
    "        regex=r'^(match_ingles|match_nivel_academico|match_area_atuacao|match_localidade|match_pcd|qtd_keywords_cv|match_cv_atividade$)'\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Combine all features\n",
    "    X_final = pd.concat([X_texto_df, df_encoded_features.reset_index(drop=True), X_numeric_binary], axis=1)\n",
    "    X_final.columns = X_final.columns.astype(str)\n",
    "\n",
    "    if is_training:\n",
    "        # For training, capture the final column names\n",
    "        final_feature_columns = X_final.columns.tolist()\n",
    "    else:\n",
    "        # For prediction, reindex to match the training columns, filling missing with 0\n",
    "        if original_feature_columns is None:\n",
    "            raise ValueError(\"original_feature_columns must be provided for prediction/test.\")\n",
    "        X_final = X_final.reindex(columns=original_feature_columns, fill_value=0)\n",
    "        final_feature_columns = original_feature_columns # Just to return consistently\n",
    "\n",
    "    return X_final, tfidf, ohe_fitted_models, final_feature_columns\n",
    "\n",
    "\n",
    "def treinar_modelo_supervisionado(df_treinamento_input):\n",
    "    \"\"\"\n",
    "    Treina um modelo supervisionado XGBoost com balanceamento de classes e clustering como feature adicional.\n",
    "\n",
    "    Esta função realiza o pré-processamento completo dos dados, incluindo:\n",
    "    - Extração de features TF-IDF e estruturadas (com One-Hot Encoding).\n",
    "    - Balanceamento das classes com SMOTEENN (over + under sampling).\n",
    "    - Ajuste dos hiperparâmetros via cross-validation com `xgb.cv`.\n",
    "    - Treinamento final com o número ideal de árvores (`n_estimators`).\n",
    "    - Avaliação com métricas de classificação e registro no MLflow.\n",
    "    - Salvamento do modelo, do vetor TF-IDF e dos OneHotEncoders via `joblib`.\n",
    "\n",
    "    Args:\n",
    "        df_treinamento_input (pd.DataFrame): DataFrame contendo os dados de treino,\n",
    "            já rotulados com a coluna 'contratado'.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[XGBClassifier, TfidfVectorizer, dict, List[str]]:\n",
    "            - O modelo XGBoost treinado.\n",
    "            - O vetorizador TF-IDF ajustado.\n",
    "            - O dicionário de OneHotEncoders ajustados.\n",
    "            - A lista de nomes das colunas finais de features.\n",
    "    \"\"\"\n",
    "    df_treinamento_input.columns = df_treinamento_input.columns.astype(str)\n",
    "\n",
    "    # Extrai e transforma features no conjunto de TREINO.\n",
    "    # tfidf e ohe_models serão ajustados aqui.\n",
    "    X, tfidf_model, ohe_models, original_feature_columns = extrair_e_transformar_features(\n",
    "        df_treinamento_input, is_training=True\n",
    "    )\n",
    "\n",
    "    y = df_treinamento_input['contratado']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    print(\"Distribuição original do treinamento:\")\n",
    "    print(y_train.value_counts())\n",
    "\n",
    "    # Apply SMOTEENN on the training set\n",
    "    smoteenn = SMOTEENN(random_state=42)\n",
    "    X_train_res, y_train_res = smoteenn.fit_resample(X_train, y_train)\n",
    "\n",
    "    print(\"\\nDistribuição após SMOTEENN no treinamento:\")\n",
    "    print(y_train_res.value_counts())\n",
    "\n",
    "    # Prepare DMatrix for xgb.cv with the resampled data\n",
    "    dtrain = DMatrix(X_train_res, label=y_train_res)\n",
    "    # X_test needs to be transformed using the fitted TF-IDF and OHE models.\n",
    "    # For now, X_test already is, as it's a split of X which was generated with the fitted models.\n",
    "    # However, if we were loading X_test from an external source, it would need the full transformation.\n",
    "    # Let's create DMatrix for X_test as well.\n",
    "    dtest = DMatrix(X_test, label=y_test)\n",
    "\n",
    "    param = {\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_delta_step': 1,\n",
    "        'nthread': 16,\n",
    "        'subsample': 0.5,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'seed': 42\n",
    "    }\n",
    "    num_round = 300  # Max rounds for CV\n",
    "\n",
    "    print(\"\\nExecutando cross-validation com xgb.cv...\")\n",
    "    cv_results = cv(\n",
    "        param,\n",
    "        dtrain,\n",
    "        num_boost_round=num_round,\n",
    "        nfold=5,\n",
    "        seed=42,\n",
    "        metrics=['auc'],\n",
    "        fpreproc=fpreproc,\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=10\n",
    "    )\n",
    "\n",
    "    best_num_round = len(cv_results) if len(cv_results) > 0 else num_round\n",
    "    print(f\"\\nMelhor número de rounds: {best_num_round}\")\n",
    "\n",
    "    clf = XGBClassifier(\n",
    "        max_depth=param['max_depth'],\n",
    "        learning_rate=param['learning_rate'],\n",
    "        max_delta_step=param['max_delta_step'],\n",
    "        n_estimators=best_num_round,\n",
    "        nthread=param['nthread'],\n",
    "        subsample=param['subsample'],\n",
    "        colsample_bytree=param['colsample_bytree'],\n",
    "        # scale_pos_weight is calculated based on the RESAMPLED data for the final model fit\n",
    "        scale_pos_weight=float(np.sum(y_train_res == 0)) / np.sum(y_train_res == 1),\n",
    "        objective=param['objective'],\n",
    "        eval_metric=param['eval_metric'],\n",
    "        use_label_encoder=False,\n",
    "        random_state=42\n",
    "    )\n",
    "    # Fit the model on the RESAMPLED training data!\n",
    "    clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(\"\\nClassification Report (Padrão 0.5):\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix (Padrão 0.5):\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nROC AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "\n",
    "    mlflow.set_experiment(\"modelo_candidato_sucesso\")\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(clf.get_params())\n",
    "        mlflow.log_metric(\"acuracia\", clf.score(X_test, y_test))\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))\n",
    "        mlflow.log_metric(\"precision_class1\", precision_score(y_test, y_pred, pos_label=1))\n",
    "        mlflow.log_metric(\"recall_class1\", recall_score(y_test, y_pred, pos_label=1))\n",
    "        mlflow.log_metric(\"f1_score_class1\", f1_score(y_test, y_pred, pos_label=1))\n",
    "\n",
    "        importances = clf.feature_importances_\n",
    "        feature_names = original_feature_columns\n",
    "        fi_df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
    "        fi_df.to_csv(\"feature_importances.csv\", index=False)\n",
    "        mlflow.log_artifact(\"feature_importances.csv\")\n",
    "\n",
    "        # Use the raw X_test (before DMatrix conversion if needed) as input_example\n",
    "        input_example = X_test.iloc[:1]\n",
    "        signature = infer_signature(X_test, clf.predict(X_test))\n",
    "        mlflow.sklearn.log_model(clf, \"modelo_xgboost\", input_example=input_example, signature=signature)\n",
    "\n",
    "    joblib.dump(clf, \"modelo_xgboost.pkl\")\n",
    "    joblib.dump(tfidf_model, \"vetorizador_tfidf.pkl\")  # Save the fitted tfidf vectorizer\n",
    "    joblib.dump(ohe_models, \"one_hot_encoders.pkl\") # Save the dictionary of fitted OHE models\n",
    "    joblib.dump(original_feature_columns, \"feature_columns.pkl\") # Save the list of feature names\n",
    "\n",
    "    return clf, tfidf_model, ohe_models, original_feature_columns\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"C:\\\\Users\\\\ffporto\\\\Desktop\\\\Estudo\\\\FIAP\\\\fase05\\\\data\\\\\"\n",
    "    # Ensure dataset_processado.parquet is generated by the new preprocess.py first\n",
    "    df = carregar_dados(f\"{path}dataset_processado.parquet\")\n",
    "    df.columns = df.columns.astype(str)\n",
    "\n",
    "    # 1. Prepare training data and \"in-progress\" data\n",
    "    # This separation happens on the raw, pre-processed (text/direct features) dataframe\n",
    "    df_treinamento, df_em_andamento = criar_coluna_contratado_refinada(df)\n",
    "\n",
    "    # 2. Train the model and get all fitted transformers and feature columns\n",
    "    clf, tfidf_model, ohe_models, original_feature_columns = treinar_modelo_supervisionado(df_treinamento)\n",
    "\n",
    "    # 3. Prepare \"in-progress\" data for prediction using the loaded/fitted transformers\n",
    "    X_em_andamento, _, _, _ = extrair_e_transformar_features(\n",
    "        df_em_andamento,\n",
    "        tfidf_model=tfidf_model,\n",
    "        ohe_models=ohe_models,\n",
    "        original_feature_columns=original_feature_columns,\n",
    "        is_training=False # IMPORTANT: Set to False for prediction/test\n",
    "    )\n",
    "\n",
    "    # 4. Make probability predictions for \"in-progress\" candidates\n",
    "    probabilities_em_andamento = clf.predict_proba(X_em_andamento)[:, 1]\n",
    "\n",
    "    # 5. Add predictions back to the 'df_em_andamento' DataFrame\n",
    "    df_em_andamento['prob_contratado'] = probabilities_em_andamento\n",
    "\n",
    "    # 6. Classify with an adjusted threshold for actionable insights\n",
    "    # Adjust this threshold based on your desired balance of precision and recall for 'contratado'\n",
    "    threshold_predicao = 0.5  # Example: You might want to experiment with this value\n",
    "    df_em_andamento['predicao_contratado'] = (df_em_andamento['prob_contratado'] > threshold_predicao).astype(int)\n",
    "\n",
    "    print(\"\\n--- Candidatos Em Andamento com Previsões ---\")\n",
    "    # Display top 10 candidates with highest probability of being hired\n",
    "    print(df_em_andamento[['situacao_candidado', 'prob_contratado', 'predicao_contratado']].sort_values(\n",
    "        by='prob_contratado', ascending=False).head(10))\n",
    "\n",
    "    # You can save this DataFrame with predictions for further analysis\n",
    "    df_em_andamento.to_parquet(f\"{path}dataset_em_andamento_com_predicao.parquet\", index=False)\n",
    "    print(\"\\nModelos treinados e salvos com sucesso! Previsões para candidatos em andamento geradas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2893023-a1bf-438f-a2df-93b21959b088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
